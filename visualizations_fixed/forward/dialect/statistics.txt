================================================================================
Statistics for Dialect Translations
Direction: Forward (English -> Arabic)
================================================================================


================================================================================
SCORE TABLES
================================================================================


Dialect BLEU Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Jais-2-70B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  2.4345         5.1446         2.7337         2.4249         1.5417         3.0516         2.2360         2.7405         1.8933         2.7360         2.6404         2.2673         1.7199         3.3040         2.8376
QAraC.test.glf.0.qa.en                 2.7556         5.0895         3.0441         2.3312         1.6351         3.4866         2.3999         3.1256         2.1779         2.9831         2.8863         2.6083         2.0172         3.5842         3.3327
bible.dev.mgr.0.ma.en                  0.7310         2.1495         0.8981         0.2768         0.4990         0.8473         0.6217         1.9546         0.4237         0.5767         1.3804         1.7184         0.7852         2.9038         2.2675
bible.dev.mgr.0.tn.en                  1.2372         3.0879         1.3474         0.3487         0.9993         2.1159         1.1229         2.3835         0.5334         1.0032         2.5269         2.2726         0.9503         2.8962         2.4485
bible.dev.msa.0.ms.en                  7.2362         7.5482         6.3549         6.2727         3.4084         6.1342         6.6624         7.3470         4.1230         6.6637         8.6886         8.6613         4.8889         8.1084         7.8676
bible.dev.msa.1.ms.en                  2.8274         6.2170         2.9232         2.8139         1.4893         4.1169         2.9478         3.8728         2.6537         3.3834         4.2378         4.6392         1.7637         3.8737         3.5543
bible.test.mgr.0.ma.en                 0.5134         2.2150         0.9150         0.1741         0.2391         0.6391         0.4045         1.8156         0.5552         0.3136         1.1713         1.3421         0.4771         2.7317         2.0906
bible.test.mgr.0.tn.en                 0.9586         2.3494         0.9548         0.5968         0.7885         1.7163         1.2918         2.0061         0.9062         1.0136         2.4254         1.9451         0.8533         3.0637         2.3891
bible.test.msa.0.ms.en                 6.4163         7.2357         5.3667         5.5933         3.4169         5.0993         6.2401         7.4536         3.8285         5.4366         7.8186         7.3720         3.8308         7.0594         7.0272
bible.test.msa.1.ms.en                 2.3355         4.5892         2.5856         2.3450         0.9722         3.3336         2.2688         2.7930         2.4536         2.5898         3.7042         3.9017         0.9911         2.7134         2.8129
madar.dev.glf.0.qa.en                  5.3130        17.9039         6.2702         5.4792         3.7289         8.3296         4.7353        12.3651         4.3713         5.4853         5.0507         6.4567         6.3342        15.9816         8.3809
madar.dev.lev.0.lb.en                  3.4646        42.2421         5.9610         2.6533         2.1616         6.7335         2.5703         9.3064         2.4788         3.2931         8.3032         9.1806         5.0198        12.7283        11.3232
madar.dev.mgr.0.ma.en                  2.2617        30.4791         3.6791         1.1298         1.6382         4.2479         2.2055         5.7560         1.9054         3.0737         8.2143         5.9819         2.4890        10.0458         8.2828
madar.dev.mgr.0.tn.en                  2.7327        47.6776         3.8042         1.1874         1.9948         4.4815         2.4310         4.8891         2.0901         2.7084         6.3355         4.8540         2.6144         9.2611         7.5396
madar.dev.msa.0.ms.en                 17.2613        29.1713        17.0597        15.9094        11.4312        19.6651        17.4377        18.4723        13.9310        18.4239        20.1442        18.6582        15.7108        20.2427        18.6138
madar.dev.nil.0.eg.en                  4.9055        52.0877         8.1946         5.2613         2.5609        11.1001         4.2230        10.8104         3.1864         8.5337         4.6045        11.8310         7.0694        15.8052        13.1560
madar.test.glf.0.iq.en                 4.9737        12.2542         5.3948         4.3760         3.2583         6.7325         4.4109         8.1553         3.2322         5.3990         5.4798         9.3717         5.3273        12.2225         9.4985
madar.test.glf.0.om.en                18.9556        10.7863        17.3093        13.2361         8.9497        16.9895        17.8516        13.8761        13.0614        18.9940        17.7815        18.3651        15.1652        16.6275        17.0885
madar.test.glf.0.qa.en                 5.4486        17.8655         6.6935         5.6764         4.1751         9.0943         5.2852        12.5632         4.5291         6.1218         5.6432         6.7620         6.5762        16.4629         9.0667
madar.test.glf.0.sa.en                 9.1721        17.6910         9.0488         8.3069         6.6768         8.8456         8.4513        10.3475         7.7237        10.1750         8.3791        10.1567         9.3025        15.7981        13.9004
madar.test.glf.0.ye.en                 5.4525        10.9390         5.9099         5.2165         3.9437         5.4272         4.4760         6.7438         3.9534         5.7380         5.6505         6.9280         5.1450        10.1072         8.7228
madar.test.glf.1.iq.en                 4.5280        10.8618         4.5681         3.8046         2.5107         5.1521         3.3951         7.2039         2.6954         4.5766         4.3292         8.0154         3.6913        10.8540         8.0837
madar.test.glf.1.sa.en                 4.5270        13.7267         4.4950         4.4949         3.3980         4.9291         4.6263         5.5144         4.1547         5.4566         4.3017         5.3429         5.3828         8.6030         7.7760
madar.test.glf.2.iq.en                 3.6360         5.9788         3.7044         3.4220         2.7083         4.2230         3.1876         4.8156         2.3496         3.8734         3.5036         5.4429         3.3311         6.3458         5.5010
madar.test.lev.0.jo.en                 5.1800        16.5748         9.1257         5.7774         3.4506        10.6929         4.8492        15.5858         4.6407         6.2058         0.0655        14.5330         7.1256        19.8333        15.3103
madar.test.lev.0.lb.en                 3.4038        35.4440         4.9571         2.5758         2.1053         6.3272         2.7663         8.3186         2.5706         3.2124         8.4141         8.1517         4.2347        12.0381         9.3662
madar.test.lev.0.pa.en                 4.8356        18.1846         9.4142         5.6166         3.1796         7.2680         4.8108        14.1136         4.1322         6.1669         6.7718         9.3070         6.2500        19.6689        16.1900
madar.test.lev.0.sy.en                 4.2830        18.6459         6.3563         4.6272         3.0184         4.9921         4.0363         8.2252         3.6263         5.3989         4.4464        10.7111         5.2832        21.2802        16.6087
madar.test.lev.1.jo.en                 5.0561        16.0444         8.1600         5.4515         3.8194         8.6708         4.6918        12.3063         4.4372         5.7071         7.3664        11.5897         5.4567        16.1228        12.5860
madar.test.lev.1.sy.en                 4.9369        18.2744         6.6298         4.8988         3.2838         5.4808         4.4048         8.1860         3.4118         5.5772         4.4172         9.5880         4.9483        18.3300        14.8730
madar.test.mgr.0.dz.en                 6.7476         6.3982         6.1878         3.0425         3.5111         8.3277         6.5493         8.3907         4.6725         7.0059         6.6853         8.2712         4.5125        11.4069         9.6515
madar.test.mgr.0.ly.en                 4.2814         9.6307         6.1235         4.9380         1.9877         6.7637         3.9744         8.1351         2.7543         4.7450         4.8987         7.7097         4.5676         8.9969         7.7378
madar.test.mgr.0.ma.en                 3.6010        33.2688         5.1420         1.8062         2.1811         6.2903         3.2004         7.9770         2.8179         4.5853        10.5069         7.1114         3.2987        13.6943        11.5278
madar.test.mgr.0.tn.en                 2.7392        34.7214         3.2828         1.2722         1.8023         3.9715         2.2717         4.2966         1.9580         2.7101         5.4016         4.5879         2.3476         7.9380         6.4140
madar.test.mgr.1.ly.en                 6.6067        12.8406         8.2071         6.8181         4.0047         8.9693         5.3362        10.9511         5.0481         6.6405         7.1610        11.2755         6.9055        13.1602        11.1673
madar.test.mgr.1.ma.en                 7.4461        13.0018         6.9449         3.2076         4.1709         8.9421         6.6867         9.4332         5.5840         8.5362        12.0352         9.5504         5.4789        14.7528        13.4046
madar.test.mgr.1.tn.en                 2.2729         8.9953         2.5918         1.1340         1.5468         3.8182         2.0826         3.6333         1.5950         2.3128         4.7471         3.5885         1.8051         6.2830         5.2233
madar.test.msa.0.ms.en                16.8102        26.3374        16.4894        15.2788        10.7044        18.9525        17.5177        18.4081        13.6699        17.9485        19.9663        17.9171        15.2732        19.7871        18.3698
madar.test.nil.0.eg.en                 6.2396        45.5063        10.6461         6.2571         3.7907        15.2893         4.8880        15.6873         4.3337        11.2147         5.8228        16.3548        10.4557        21.0634        17.7878
madar.test.nil.0.sd.en                 9.9791        13.0352         9.6701         9.8024         5.5864        12.7939         9.7459        11.3658         6.1541        10.4769         9.9125        11.6716         8.0784        17.3772        16.0650
madar.test.nil.1.eg.en                 5.1898        14.6967         9.1746         5.8627         3.4649        12.5620         5.1227        13.3048         3.9756         9.0071         5.2464        13.0012         7.7346        17.6375        15.2942
madar.test.nil.2.eg.en                 4.7026        17.2661         9.4674         5.3204         2.7257        13.0643         3.7693        14.0239         4.4969         8.9733         5.0994        14.4328         9.1132        16.8333        14.0866



Dialect CHRF Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Jais-2-70B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                 28.6500        35.2465        29.6631        27.2288        24.8174        32.0002        28.5129        30.5558        25.1067        29.4193        30.9793        30.1604        26.8737        32.8423        31.6918
QAraC.test.glf.0.qa.en                29.1633        35.7724        30.2011        27.6905        25.1185        32.3683        28.9753        31.0472        25.6700        29.8245        31.3917        30.5202        27.4739        33.1643        32.0481
bible.dev.mgr.0.ma.en                 21.3684        26.7237        24.5501        12.0607        18.9187        21.9786        21.8427        26.4697        17.8939        19.6703        24.1846        27.6597        20.4246        31.4743        29.3885
bible.dev.mgr.0.tn.en                 21.9756        30.7248        24.2104        11.4318        20.4296        25.9911        23.6564        26.6570        18.2565        21.6879        29.8001        27.2921        21.2597        29.7686        29.3386
bible.dev.msa.0.ms.en                 35.7136        34.4671        32.1563        32.9166        28.2369        29.9837        35.6499        37.1242        24.0649        32.6149        36.1006        36.2217        31.8003        38.0651        37.3600
bible.dev.msa.1.ms.en                 29.2559        32.2822        27.4421        28.3455        23.1746        27.8688        29.4099        31.5258        22.5782        27.5489        30.4711        32.5490        26.1420        31.5510        30.8202
bible.test.mgr.0.ma.en                21.2529        28.6526        24.1187        11.7700        18.0856        21.4299        21.1019        26.8409        17.6746        19.4356        23.0702        27.7038        20.0179        31.6915        29.6195
bible.test.mgr.0.tn.en                20.7010        29.3863        22.4329        11.4987        19.5353        23.9239        22.1501        25.5372        18.1675        20.9167        28.2764        25.7539        20.0064        29.9566        28.3570
bible.test.msa.0.ms.en                35.3407        34.8032        31.4555        31.6232        27.4544        28.0850        34.6987        36.2975        24.1702        31.7804        34.9893        35.4619        30.6498        36.2138        36.4675
bible.test.msa.1.ms.en                28.7402        31.9429        26.8951        27.2922        22.5272        25.8170        28.4000        30.8473        22.6322        26.9565        30.2109        31.1514        25.3190        30.1975        30.3060
madar.dev.glf.0.qa.en                 32.9394        48.6929        34.6501        31.2882        28.3434        38.1840        32.5631        39.8972        30.4533        33.5446        34.5075        35.6717        32.9636        46.7167        38.1031
madar.dev.lev.0.lb.en                 29.9638        65.7268        32.5437        23.1197        25.2768        35.8045        28.8472        38.7271        27.4777        29.6478        38.7315        38.5377        30.6957        43.8725        41.9770
madar.dev.mgr.0.ma.en                 23.2302        52.2361        26.9770        13.9466        18.9725        28.5852        23.2506        31.6629        22.3368        25.2282        36.2508        32.7103        23.4557        40.1759        37.2078
madar.dev.mgr.0.tn.en                 23.9518        68.3541        25.9857        12.7332        20.9094        28.5949        24.0205        29.3799        22.7661        24.2496        33.2815        29.3287        22.5667        37.4094        34.5573
madar.dev.msa.0.ms.en                 49.5961        59.9861        48.6670        46.8185        40.3009        52.2580        49.8352        51.3139        44.3543        50.7070        53.1278        51.1775        47.0498        53.0367        51.3229
madar.dev.nil.0.eg.en                 30.1285        71.4377        35.1337        29.0980        24.2219        40.8845        29.0185        40.0605        27.4984        35.1410        31.2834        41.4787        33.9160        46.4403        43.2385
madar.test.glf.0.iq.en                32.3615        41.7461        33.5163        31.9081        28.4154        35.9115        32.6460        37.4006        29.9667        33.5452        35.4278        39.2009        28.9842        42.8224        40.4300
madar.test.glf.0.om.en                46.9197        38.1743        44.5344        38.4385        32.8564        45.4669        46.1648        40.2969        39.8891        46.4857        47.6229        46.1842        42.6434        44.5217        45.2393
madar.test.glf.0.qa.en                32.8929        48.2697        34.5460        31.1556        28.0529        38.5727        32.5841        40.1690        30.3463        33.4836        34.6681        35.7464        32.8765        46.6668        38.3931
madar.test.glf.0.sa.en                39.8694        46.9553        39.2662        38.0613        34.0010        40.8141        39.5066        41.3759        36.5926        40.5537        40.9049        41.3732        39.0858        48.0038        45.7678
madar.test.glf.0.ye.en                31.6484        40.5617        32.5761        30.5807        26.1397        32.7953        31.1343        34.7909        29.3016        31.7122        34.4531        35.0493        31.4243        40.4194        38.2395
madar.test.glf.1.iq.en                31.8648        41.7175        33.5160        31.0329        27.9437        34.5609        31.7912        37.0406        29.3052        32.8551        34.3735        38.6826        28.1932        42.9084        40.6003
madar.test.glf.1.sa.en                31.7979        44.3638        31.6348        31.1555        27.7971        32.6466        31.5484        33.1381        30.0688        32.2944        32.7401        33.2620        32.7949        39.1312        37.9117
madar.test.glf.2.iq.en                29.5011        33.5196        30.2751        28.9232        26.5928        31.7705        29.8495        31.4362        27.5354        30.8071        32.1071        32.8951        25.4967        35.6748        34.8445
madar.test.lev.0.jo.en                31.6909        45.0002        36.0747        31.5724        27.5957        38.0190        31.3492        42.6149        30.1687        32.3673         9.2696        42.4156        33.8197        47.9749        44.1841
madar.test.lev.0.lb.en                27.9333        61.0640        30.7570        21.9289        23.8985        33.9042        27.2505        36.6270        26.5457        28.0031        37.3235        36.7247        29.0145        41.8583        38.5848
madar.test.lev.0.pa.en                31.0180        47.6253        36.6236        30.8275        25.9276        34.5225        30.2515        42.2091        29.0511        32.0033        34.9211        37.8445        32.2175        49.3601        45.6334
madar.test.lev.0.sy.en                32.1344        49.2658        34.7665        31.5953        26.2426        34.1935        32.0998        36.9827        29.7986        33.1558        33.7171        40.8749        32.9877        51.3337        47.4682
madar.test.lev.1.jo.en                32.5830        45.6644        36.7258        32.3628        28.1915        37.9530        32.3620        41.4169        31.1248        33.5841        37.5483        41.4432        33.2814        47.6552        43.6253
madar.test.lev.1.sy.en                31.6955        48.0600        34.4588        31.4521        26.5967        33.8509        31.5300        36.3509        29.3246        32.6497        33.0901        39.2398        32.1568        48.8072        45.4053
madar.test.mgr.0.dz.en                30.2039        29.4777        29.7673        14.8937        24.1172        33.9769        30.7865        32.9741        28.2313        30.8080        33.4624        34.7355        26.5260        39.4434        37.7802
madar.test.mgr.0.ly.en                28.2525        38.4545        31.7333        27.0986        21.3558        33.7970        28.0589        35.6102        25.6588        28.7274        31.6124        35.7272        28.2175        38.4487        36.8638
madar.test.mgr.0.ma.en                27.5810        54.4729        30.1729        15.9167        21.2696        32.9037        27.6785        34.9049        26.1604        29.3650        40.4582        35.3305        26.2617        44.2573        41.9906
madar.test.mgr.0.tn.en                23.1108        58.2080        25.4253        12.7144        20.2660        27.4620        23.1990        28.1115        21.9146        23.5822        31.6343        28.1545        21.8027        36.2168        33.4337
madar.test.mgr.1.ly.en                31.6300        40.1124        34.5266        29.7369        24.1893        36.9226        31.3514        38.5817        29.5260        32.4128        35.3251        39.3885        30.8343        42.4731        40.8920
madar.test.mgr.1.ma.en                33.4151        40.1881        32.8948        17.8621        23.9896        36.7909        33.2279        36.7020        30.9610        34.6284        42.4655        37.6003        30.0897        45.1714        44.9946
madar.test.mgr.1.tn.en                22.5995        38.2858        24.8359        12.6607        20.1175        27.0649        22.9789        27.3997        21.6653        23.1920        30.3857        27.5685        21.4993        34.0932        31.6635
madar.test.msa.0.ms.en                48.5361        57.2250        47.6232        45.6809        38.8632        51.2118        49.0232        50.3973        43.7026        49.4323        52.4520        50.2318        46.1649        52.1378        50.4100
madar.test.nil.0.eg.en                31.3654        66.8771        37.5875        30.2590        25.3678        44.2512        29.8685        43.9242        28.7723        37.3885        32.4739        45.1984        36.8383        50.1913        46.9155
madar.test.nil.0.sd.en                37.5231        40.6609        36.9295        35.8970        29.5762        41.6463        37.9779        39.2739        32.9591        38.3671        39.8944        40.0770        34.3207        46.7088        45.4156
madar.test.nil.1.eg.en                31.2885        46.3604        37.1417        29.9855        25.4982        43.3338        30.2423        42.5320        29.1361        36.1537        32.5179        44.0711        35.2788        49.2073        46.2362
madar.test.nil.2.eg.en                28.9448        46.6818        35.6293        28.7862        23.9367        41.3608        27.7885        41.5140        28.0171        34.7385        30.6076        42.8341        35.1832        46.0995        43.3264


================================================================================
ðŸ“Š Average BLEU Scores for Dialect Translations
================================================================================

Models sorted by average BLEU score:

Model: Jais-2-70B-Chat
  Average BLEU: 17.2418
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average BLEU: 11.8459
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average BLEU: 9.8863
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average BLEU: 8.5394
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average BLEU: 8.4149
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average BLEU: 7.3731
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average BLEU: 6.5278
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average BLEU: 6.3759
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average BLEU: 6.0709
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average BLEU: 5.3426
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average BLEU: 5.3406
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average BLEU: 4.9568
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average BLEU: 4.6909
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average BLEU: 4.0259
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average BLEU: 3.2967
  Based on 42 test set(s)

================================================================================
ðŸ“Š Average CHRF Scores for Dialect Translations
================================================================================

Models sorted by average CHRF score:

Model: Jais-2-70B-Chat
  Average CHRF: 44.6530
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average CHRF: 41.5277
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average CHRF: 39.2394
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average CHRF: 36.5532
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average CHRF: 36.1362
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average CHRF: 34.5110
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average CHRF: 34.2408
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average CHRF: 32.6338
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average CHRF: 31.6826
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average CHRF: 30.9603
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average CHRF: 30.8139
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average CHRF: 30.2050
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average CHRF: 27.7815
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average CHRF: 26.6988
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average CHRF: 25.5981
  Based on 42 test set(s)

================================================================================
ðŸ“Š Ranking Statistics for Dialect BLEU Score
================================================================================
Total test sets with Dialect BLEU scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  2nd: 1 time
  7th: 3 times
  8th: 2 times
  9th: 9 times
  10th: 11 times
  11th: 7 times
  12th: 9 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 25 times
  2nd: 8 times
  3rd: 4 times
  4th: 2 times
  5th: 1 time
  10th: 1 time
  14th: 1 time
  Win rate: 25/42 (59.5%)
  Top-3 rate: 37/42 (88.1%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  5th: 1 time
  6th: 5 times
  7th: 10 times
  8th: 13 times
  9th: 3 times
  10th: 3 times
  11th: 6 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  8th: 1 time
  9th: 1 time
  10th: 11 times
  11th: 8 times
  12th: 7 times
  13th: 3 times
  15th: 11 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  12th: 1 time
  13th: 1 time
  14th: 12 times
  15th: 28 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  3rd: 2 times
  4th: 7 times
  5th: 3 times
  6th: 10 times
  7th: 10 times
  8th: 5 times
  9th: 1 time
  10th: 2 times
  12th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: aya-expanse-8b
  Total appearances: 42/42
  4th: 1 time
  8th: 2 times
  9th: 6 times
  10th: 1 time
  11th: 6 times
  12th: 9 times
  13th: 16 times
  14th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 1 time
  3rd: 5 times
  4th: 8 times
  5th: 17 times
  6th: 9 times
  7th: 1 time
  11th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  9th: 1 time
  10th: 1 time
  12th: 3 times
  13th: 11 times
  14th: 24 times
  15th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  1st: 1 time
  5th: 2 times
  6th: 2 times
  7th: 10 times
  8th: 7 times
  9th: 9 times
  10th: 5 times
  11th: 4 times
  12th: 1 time
  13th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 1/42 (2.4%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 2 times
  2nd: 2 times
  3rd: 4 times
  4th: 7 times
  5th: 1 time
  6th: 3 times
  7th: 1 time
  8th: 7 times
  9th: 2 times
  10th: 2 times
  11th: 4 times
  12th: 5 times
  13th: 1 time
  15th: 1 time
  Win rate: 2/42 (4.8%)
  Top-3 rate: 8/42 (19.0%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  2nd: 3 times
  3rd: 4 times
  4th: 8 times
  5th: 13 times
  6th: 9 times
  7th: 2 times
  8th: 1 time
  10th: 1 time
  11th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 7/42 (16.7%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  6th: 1 time
  7th: 2 times
  8th: 4 times
  9th: 9 times
  10th: 4 times
  11th: 5 times
  12th: 4 times
  13th: 9 times
  14th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 14 times
  2nd: 22 times
  3rd: 2 times
  5th: 2 times
  7th: 1 time
  9th: 1 time
  Win rate: 14/42 (33.3%)
  Top-3 rate: 38/42 (90.5%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  2nd: 5 times
  3rd: 21 times
  4th: 9 times
  5th: 2 times
  6th: 3 times
  7th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 26/42 (61.9%)

================================================================================
ðŸ“Š Ranking Statistics for Dialect CHRF Score
================================================================================
Total test sets with Dialect CHRF scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  2nd: 1 time
  5th: 1 time
  6th: 1 time
  7th: 1 time
  8th: 1 time
  9th: 4 times
  10th: 15 times
  11th: 17 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 21 times
  2nd: 11 times
  3rd: 3 times
  4th: 3 times
  7th: 1 time
  8th: 1 time
  11th: 1 time
  14th: 1 time
  Win rate: 21/42 (50.0%)
  Top-3 rate: 35/42 (83.3%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  6th: 5 times
  7th: 6 times
  8th: 18 times
  9th: 2 times
  10th: 1 time
  11th: 9 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  9th: 3 times
  10th: 1 time
  11th: 1 time
  12th: 11 times
  13th: 13 times
  15th: 13 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  13th: 4 times
  14th: 14 times
  15th: 24 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  3rd: 3 times
  4th: 5 times
  5th: 3 times
  6th: 10 times
  7th: 13 times
  8th: 4 times
  10th: 1 time
  12th: 1 time
  13th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 3/42 (7.1%)

Model: aya-expanse-8b
  Total appearances: 42/42
  5th: 1 time
  7th: 2 times
  8th: 3 times
  9th: 6 times
  10th: 9 times
  11th: 5 times
  12th: 11 times
  13th: 4 times
  14th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 1 time
  3rd: 4 times
  4th: 4 times
  5th: 14 times
  6th: 15 times
  7th: 3 times
  11th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 5/42 (11.9%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  12th: 6 times
  13th: 9 times
  14th: 24 times
  15th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  3rd: 1 time
  7th: 2 times
  8th: 9 times
  9th: 20 times
  10th: 5 times
  11th: 3 times
  12th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 1 time
  2nd: 3 times
  3rd: 1 time
  4th: 8 times
  5th: 6 times
  6th: 5 times
  7th: 9 times
  8th: 4 times
  10th: 4 times
  15th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 5/42 (11.9%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  1st: 1 time
  2nd: 1 time
  3rd: 2 times
  4th: 18 times
  5th: 12 times
  6th: 4 times
  7th: 4 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 4/42 (9.5%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  6th: 1 time
  8th: 2 times
  9th: 6 times
  10th: 6 times
  11th: 5 times
  12th: 9 times
  13th: 10 times
  14th: 2 times
  15th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 18 times
  2nd: 17 times
  3rd: 5 times
  6th: 1 time
  9th: 1 time
  Win rate: 18/42 (42.9%)
  Top-3 rate: 40/42 (95.2%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 1 time
  2nd: 8 times
  3rd: 23 times
  4th: 4 times
  5th: 5 times
  7th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 32/42 (76.2%)

================================================================================
ðŸ“Š Average Rankings for Dialect BLEU Score
================================================================================
Lower rank is better (1st = best)

1. Jais-2-70B-Chat: 2.14 (based on 42 test set(s))
2. gpt-4.1-mini: 2.14 (based on 42 test set(s))
3. gpt-4.1-nano: 3.60 (based on 42 test set(s))
4. c4ai-command-r-08-2024: 4.90 (based on 42 test set(s))
5. gemma-3-27b-it: 5.05 (based on 42 test set(s))
6. aya-expanse-32b: 6.48 (based on 42 test set(s))
7. command-a-translate-08-2025: 7.19 (based on 42 test set(s))
8. Llama-3.3-70B-Instruct: 8.19 (based on 42 test set(s))
9. c4ai-command-r7b-arabic-02-2025: 8.31 (based on 42 test set(s))
10. EuroLLM-9B-Instruct: 9.88 (based on 42 test set(s))
11. gemma-3-4b-it: 10.69 (based on 42 test set(s))
12. aya-expanse-8b: 11.43 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 11.98 (based on 42 test set(s))
14. c4ai-command-r-v01: 13.43 (based on 42 test set(s))
15. Qwen3-4B-Instruct-2507: 14.60 (based on 42 test set(s))


================================================================================
ðŸ“Š Average Rankings for Dialect CHRF Score
================================================================================
Lower rank is better (1st = best)

1. gpt-4.1-mini: 1.95 (based on 42 test set(s))
2. Jais-2-70B-Chat: 2.48 (based on 42 test set(s))
3. gpt-4.1-nano: 3.19 (based on 42 test set(s))
4. gemma-3-27b-it: 4.60 (based on 42 test set(s))
5. c4ai-command-r-08-2024: 5.29 (based on 42 test set(s))
6. command-a-translate-08-2025: 6.00 (based on 42 test set(s))
7. aya-expanse-32b: 6.55 (based on 42 test set(s))
8. Llama-3.3-70B-Instruct: 8.45 (based on 42 test set(s))
9. c4ai-command-r7b-arabic-02-2025: 8.95 (based on 42 test set(s))
10. EuroLLM-9B-Instruct: 9.83 (based on 42 test set(s))
11. aya-expanse-8b: 10.48 (based on 42 test set(s))
12. gemma-3-4b-it: 11.24 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 12.95 (based on 42 test set(s))
14. c4ai-command-r-v01: 13.57 (based on 42 test set(s))
15. Qwen3-4B-Instruct-2507: 14.48 (based on 42 test set(s))

