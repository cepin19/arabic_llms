================================================================================
Statistics for General Arabic Translations
Direction: Forward (English -> Arabic)
================================================================================


================================================================================
SCORE TABLES
================================================================================


General Arabic BLEU Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  2.0866            N/A         2.8212         1.5726         2.4957         1.9262         1.4448         2.3126         2.0522         2.1896         1.9010         2.4384         2.2657         2.1624         1.7562         2.7395         2.3909
QAraC.test.glf.0.qa.en                 2.3405            N/A         2.9469         1.7164         2.7293         1.9730         1.4595         2.4661         2.2311         2.3688         2.0498         2.6287         2.5167         2.3846         1.8338         2.8313         2.6698
bible.dev.mgr.0.ma.en                  0.8021            N/A         0.7153         0.5603         0.7480         0.5257         0.4168         0.8174         0.7819         0.7309         0.5145         0.7411         0.8287         0.8144         0.5939         0.8797         0.8370
bible.dev.mgr.0.tn.en                  1.5932            N/A         1.4489         1.1580         1.2110         0.9245         0.9640         1.5412         1.4386         1.5356         0.4510         1.1159         1.3611         1.6796         0.8728         1.6248         1.6811
bible.dev.msa.0.ms.en                  6.8908            N/A         8.1926         4.6943         6.6758         4.6400         3.4803         6.4519         6.9360         7.1693         4.3907         6.3658         7.9428         8.6003         5.1943         8.4088         7.7403
bible.dev.msa.1.ms.en                  2.9003            N/A         6.0147         1.5800         2.9794         2.1806         1.4094         3.8311         3.0965         4.2426         2.5326         3.1501         0.1257         4.7045         1.6686         3.7226         3.4135
bible.test.mgr.0.ma.en                 0.5384            N/A         0.5269         0.3258         0.5907         0.3244         0.2185         0.6374         0.5048         0.6789         0.5912         0.3146         0.3673         0.7624         0.2589         0.5512         0.4348
bible.test.mgr.0.tn.en                 1.5177            N/A         1.1218         0.8650         1.0250         1.1433         0.7041         1.2266         1.4740         1.2004         0.8916         0.9954         1.0788         1.1800         0.8193         1.3893         1.4272
bible.test.msa.0.ms.en                 6.2370            N/A         7.3194         4.6766         5.5802         4.8432         3.4386         6.1591         6.2604         7.5092         4.3844         5.4597         7.3589         7.9193         4.2117         7.6461         6.8272
bible.test.msa.1.ms.en                 2.0442            N/A         4.6909         1.5614         2.5493         2.1124         1.0820         2.5897         2.1974         3.0906         2.4897         2.4781         3.0915         3.8547         1.2009         2.7074         2.7262
madar.dev.glf.0.qa.en                  4.6563            N/A         5.0572         4.1360         4.5620         3.4303         3.0152         4.8073         4.5002         4.8445         4.3557         4.7671         4.5666         5.2195         4.1327         5.1204         4.9925
madar.dev.lev.0.lb.en                  2.4043            N/A         2.8818         2.2858         2.5386         2.0250         1.6522         2.7093         2.4141         2.6313         2.0284         2.7176         2.2821         2.6740         2.2149         2.7600         2.6401
madar.dev.mgr.0.ma.en                  2.0764            N/A         2.3712         2.0933         2.1726         1.7006         1.7176         2.2636         2.1753         2.3859         1.7208         2.2938         1.9582         2.4246         1.9247         2.2839         2.2956
madar.dev.mgr.0.tn.en                  2.2122            N/A         2.5227         2.0626         2.4104         1.8712         1.7527         2.3782         2.3078         2.4969         1.8910         2.3648         2.0846         2.3827         2.2337         2.5649         2.5217
madar.dev.msa.0.ms.en                 17.5150            N/A        32.6326        20.8524        17.3985        14.6566        11.5274        20.4815        17.9562        19.0886        14.4109        18.7230        20.1457        19.2049        16.3788        20.6121        18.7300
madar.dev.nil.0.eg.en                  3.7710            N/A         4.3494         3.7646         4.0380         2.9601         2.1870         4.1305         3.9897         4.1967         2.7258         4.1306         3.7346         4.1553         3.7452         4.3578         4.0722
madar.test.glf.0.iq.en                 4.4091            N/A         4.5959         3.9722         4.5856         3.3780         2.9324         4.8429         4.3997         4.4317         3.2171         4.5905         3.2199         4.7267         3.7833         4.8785         4.7844
madar.test.glf.0.om.en                19.3653            N/A        18.0769        15.8640        18.1065        15.5505        11.5861        18.5099        17.9933        18.2659        13.0206        19.0060        14.4467        19.3035        15.0956        19.9553        19.5262
madar.test.glf.0.qa.en                 4.8942            N/A         5.3743         4.3094         4.8959         4.1897         3.5092         5.3085         4.9007         5.2826         4.6198         5.4815         4.8905         5.4736         4.1597         5.6289         5.4567
madar.test.glf.0.sa.en                 8.9108            N/A         8.5374         7.6442         8.5709         7.2495         6.1944         8.8076         8.4731         8.8417         7.5158         9.5097         6.2409         9.1089         7.1806         9.8925         9.5000
madar.test.glf.0.ye.en                 4.9269            N/A         4.5146         3.8943         5.0478         4.1665         3.6020         4.9393         4.5849         4.7402         3.8626         5.2743         3.6456         5.0137         3.9777         5.4731         5.2804
madar.test.glf.1.iq.en                 3.5240            N/A         3.3547         3.3061         3.3046         2.7738         2.5519         3.6565         3.3190         3.3252         2.5324         3.8187         2.6058         3.3818         2.7197         4.1231         4.0697
madar.test.glf.1.sa.en                 4.4001            N/A         4.7850         4.0039         4.3998         3.8586         3.2146         4.9433         4.5072         4.7159         4.2534         5.1261         3.7030         4.7669         3.7702         5.1971         4.8002
madar.test.glf.2.iq.en                 3.1471            N/A         3.0488         2.7426         3.2755         2.6485         2.3026         3.1952         3.2179         3.0407         2.1035         3.3798         2.4856         2.9917         2.4372         3.4333         3.5155
madar.test.lev.0.jo.en                 4.7729            N/A         4.7652         4.0938         4.6954         3.8944         3.1305         5.3096         4.5920         5.1582         4.3558         5.1711         3.7765         4.9145         3.8737         5.1548         4.8490
madar.test.lev.0.lb.en                 2.4793            N/A         3.0499         2.3445         2.5800         2.1506         1.6914         2.6915         2.5145         2.6476         2.0798         2.8093         2.3582         2.8251         1.9995         2.9389         2.7139
madar.test.lev.0.pa.en                 4.5086            N/A         4.4737         3.6391         4.8791         3.9316         3.0971         4.7511         4.7944         4.8562         4.0962         5.1020         3.4958         4.5887         3.5924         5.1013         5.0924
madar.test.lev.0.sy.en                 4.0964            N/A         4.2228         3.6066         4.0065         3.3745         2.3877         4.2654         4.1532         4.3758         3.4650         4.4840         2.8605         4.1899         3.2106         4.6121         4.1470
madar.test.lev.1.jo.en                 4.8788            N/A         4.6365         3.9849         4.5705         3.9947         3.1671         4.8415         4.3500         4.8479         4.0815         4.8732         3.2270         4.8712         3.6590         5.0364         4.9187
madar.test.lev.1.sy.en                 4.1516            N/A         4.2315         3.6464         4.1756         3.4377         2.6679         4.6458         4.3587         4.6542         3.2615         4.6623         3.2680         4.5794         3.2166         4.7852         4.4980
madar.test.mgr.0.dz.en                 6.2584            N/A         6.0443         5.4457         6.3393         5.0191         3.6331         6.5841         6.2724         6.7063         4.2324         6.7895         5.0354         6.8113         5.0784         6.8802         6.2631
madar.test.mgr.0.ly.en                 3.5168            N/A         3.2417         3.0217         3.7474         3.1057         2.1002         3.7073         3.7491         3.7492         2.7044         4.0029         3.0880         3.5090         3.0487         4.1955         3.9516
madar.test.mgr.0.ma.en                 3.0949            N/A         3.2595         2.8013         3.0098         2.5926         2.0374         3.2194         3.0397         3.2433         2.4758         3.2899         3.0809         3.0933         2.5959         3.3746         3.1611
madar.test.mgr.0.tn.en                 2.2072            N/A         2.5238         2.0376         2.3068         2.1271         1.6074         2.3633         2.1728         2.3869         1.8054         2.5121         2.2524         2.4158         1.8260         2.5325         2.3802
madar.test.mgr.1.ly.en                 5.6094            N/A         5.4800         4.7098         5.3345         4.6989         4.1235         5.6883         5.3869         5.7916         4.4876         5.9814         4.6243         5.9186         4.3818         6.0591         6.0927
madar.test.mgr.1.ma.en                 7.2605            N/A         7.3452         6.1346         6.9612         5.5691         4.9577         7.4898         6.7054         7.3069         5.1972         7.6213         5.7194         7.4306         5.3863         7.6659         7.3453
madar.test.mgr.1.tn.en                 1.9410            N/A         1.8456         1.5030         1.7482         1.4756         1.3263         2.0936         2.1058         2.0955         1.3211         2.1031         1.4330         1.8951         1.4865         2.1776         2.0543
madar.test.msa.0.ms.en                17.0775            N/A        29.3625        20.0321        16.9219        15.0959        10.9563        19.6650        17.8161        18.8881        14.1386        18.4823        19.8912        18.4538        15.7988        19.9497        18.5253
madar.test.nil.0.eg.en                 4.5468            N/A         5.3960         4.3472         4.6690         4.0708         3.0800         4.8274         4.5107         4.9920         3.8938         5.0526         4.3580         5.0253         3.9628         5.2382         4.8096
madar.test.nil.0.sd.en                 9.3277            N/A         8.1957         7.6486         9.1725         7.5772         5.5456         9.1263         9.2704         8.8218         5.8619         9.7250         6.8134         8.8037         7.6476        10.2338         9.8392
madar.test.nil.1.eg.en                 4.3760            N/A         4.6763         3.9972         4.6121         3.9605         2.4420         4.7200         4.5806         4.5639         3.4739         5.0325         3.4663         4.5403         3.6248         4.9434         4.9119
madar.test.nil.2.eg.en                 3.7238            N/A         4.0243         3.3250         3.5181         3.1626         2.3428         3.8355         3.5911         3.9889         3.7037         4.1926         2.9684         3.9256         3.0563         4.3360         4.0953



General Arabic CHRF Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                 28.3439            N/A        30.1391        26.7730        28.8461        26.8050        24.9081        29.3381        28.2420        29.0127        26.0614        28.6529        29.4562        29.2440        27.1862        29.7999        29.2832
QAraC.test.glf.0.qa.en                28.7881            N/A        30.7644        27.3050        29.3197        27.4552        25.3574        29.7216        28.5161        29.4165        26.4043        29.0331        29.8956        29.6300        27.6184        30.1990        28.5822
bible.dev.mgr.0.ma.en                 22.1647            N/A        22.3488        19.7586        21.0807        19.8504        19.6070        20.5872        22.2525        22.7450        16.7426        21.2256        20.7733        21.9983        20.8969        22.7180        22.6075
bible.dev.mgr.0.tn.en                 24.2280            N/A        24.4916        21.4009        22.8210        21.4754        20.6929        22.3886        24.0856        24.7358        17.8840        22.8955        22.4808        24.0144        22.5106        24.7696        24.6684
bible.dev.msa.0.ms.en                 35.9770            N/A        37.2149        31.0840        33.8329        31.4800        28.2545        32.7267        36.0349        37.3027        24.2598        33.8096        35.3939        36.4185        32.6450        38.6235        37.5295
bible.dev.msa.1.ms.en                 29.2529            N/A        33.7892        24.7385        28.4663        26.6992        23.1737        28.6965        29.4595        32.1631        22.4256        28.3748        12.7866        32.4698        26.2392        31.7083        30.9753
bible.test.mgr.0.ma.en                21.8321            N/A        21.5058        19.5149        20.8607        19.2343        19.0259        19.8714        21.3077        22.1135        16.5667        20.5503        20.9235        21.5066        20.0665        22.1734        22.1481
bible.test.mgr.0.tn.en                23.0369            N/A        22.6132        19.8582        21.7925        20.9957        19.7922        20.6965        22.5319        23.2885        17.3590        21.5081        21.0990        22.7041        21.2820        23.3997        23.3844
bible.test.msa.0.ms.en                35.6294            N/A        36.2819        31.5512        32.9714        31.4247        27.8249        31.5337        34.9049        37.0457        24.3555        32.7097        34.8363        36.1374        31.2808        37.1217        36.4694
bible.test.msa.1.ms.en                28.8495            N/A        32.9739        24.9054        28.0955        27.0058        22.8363        27.4894        28.2831        31.4788        22.7670        27.6686        28.1083        31.4852        25.6149        30.8209        30.3924
madar.dev.glf.0.qa.en                 32.3259            N/A        34.0642        32.0186        32.6212        29.7796        27.9916        33.2083        32.2503        33.2296        31.2703        32.6775        33.3968        33.5568        31.3986        33.9453        33.4102
madar.dev.lev.0.lb.en                 28.7912            N/A        30.3802        28.5550        28.9734        26.3742        25.1310        29.4887        28.6601        29.4882        27.0732        29.0932        28.8952        29.5110        27.7167        30.1419        29.5923
madar.dev.mgr.0.ma.en                 23.2428            N/A        23.9745        22.7447        23.2109        21.4316        20.7401        23.6086        23.1070        23.6698        22.0007        23.2927        23.8616        23.6492        22.5471        23.8051        23.4232
madar.dev.mgr.0.tn.en                 23.5446            N/A        24.7299        23.2563        23.7990        22.0913        21.3741        24.1684        23.5859        24.1503        22.5118        23.7726        24.0306        24.1497        23.1086        24.4807        24.2220
madar.dev.msa.0.ms.en                 49.7739            N/A        62.0522        53.0905        49.1148        44.7947        40.3422        52.9821        50.2466        51.9768        45.1482        51.1178        52.3634        51.8418        47.9339        53.3026        51.5435
madar.dev.nil.0.eg.en                 28.6232            N/A        30.6222        28.9010        28.7694        26.3987        24.4110        29.4057        28.6889        29.2339        26.6965        28.9624        29.2342        29.3667        27.8870        29.9406        29.1965
madar.test.glf.0.iq.en                32.9930            N/A        33.8659        32.0589        32.3709        30.4549        28.4750        33.5053        32.1848        33.0405        30.3444        32.9902        32.2067        32.9689        31.0012        33.8521        33.2737
madar.test.glf.0.om.en                47.8837            N/A        48.2807        45.5832        46.1395        43.5883        38.2772        47.3849        46.1767        46.7595        40.8050        47.1250        44.6875        47.4780        44.1002        49.1230        48.3001
madar.test.glf.0.qa.en                32.3436            N/A        34.0026        31.8884        32.3550        30.6675        27.9148        33.4492        32.2320        33.2695        31.2233        32.7112        33.3770        33.5124        30.8662        33.8027        33.2651
madar.test.glf.0.sa.en                39.8654            N/A        41.2477        39.0657        39.3126        37.1140        34.0151        40.4584        39.4823        40.2016        36.7876        40.3767        38.4533        40.2451        37.5973        42.0035        41.0548
madar.test.glf.0.ye.en                31.4809            N/A        32.3482        30.5109        31.1553        29.6517        27.5533        31.9320        31.0145        31.5540        29.7544        31.6427        29.6093        31.7906        30.0537        32.8260        32.2480
madar.test.glf.1.iq.en                32.1221            N/A        32.7383        31.1643        31.5216        29.8624        27.7339        32.4587        31.4625        31.9327        29.5463        32.1250        30.6638        31.9431        29.9225        33.0347        32.7705
madar.test.glf.1.sa.en                31.6755            N/A        32.9774        31.1470        31.6634        29.9464        27.7206        32.5253        31.4196        32.1287        30.3106        32.1569        30.3906        32.5043        30.2885        33.2733        32.6822
madar.test.glf.2.iq.en                30.4920            N/A        30.8411        29.3369        29.7996        28.1055        26.4360        30.6843        29.7368        30.2101        27.8794        30.2645        28.8475        30.2645        28.7043        31.3194        31.0000
madar.test.lev.0.jo.en                31.4302            N/A        32.5659        30.7237        30.9975        29.4357        27.3847        32.3898        30.9390        31.9508        29.7695        31.5671        30.3017        31.8273        29.8873        32.7931        32.1334
madar.test.lev.0.lb.en                27.0779            N/A        28.4582        26.5897        27.0770        25.7051        23.8994        27.9565        26.9241        27.7795        25.7295        27.2601        27.6129        28.0913        26.0344        28.3348        27.6287
madar.test.lev.0.pa.en                30.5669            N/A        31.9203        29.9082        30.5992        28.9222        26.8373        31.3384        30.3601        31.0344        29.1418        31.0238        29.8509        31.2120        29.3460        32.0954        31.6432
madar.test.lev.0.sy.en                31.9446            N/A        33.2674        31.3402        31.4873        29.8031        27.3163        32.8751        31.8552        32.7980        30.2948        32.0747        30.4754        32.8376        30.4362        33.6185        32.7549
madar.test.lev.1.jo.en                32.7639            N/A        33.5001        31.8284        31.9702        30.8210        27.8515        33.1997        31.9601        32.7525        30.7341        32.7689        30.4604        32.8958        30.5723        33.7699        33.4892
madar.test.lev.1.sy.en                31.2241            N/A        32.8274        30.8830        31.0877        29.4552        27.0753        32.3707        31.1161        32.1982        29.7905        31.7269        30.6914        32.1067        29.7574        33.1670        32.3055
madar.test.mgr.0.dz.en                30.5880            N/A        31.2695        29.8506        30.2381        28.7526        25.6362        31.6243        30.4843        31.2326        27.6077        30.6540        29.4574        31.1045        28.9124        31.6860        30.7648
madar.test.mgr.0.ly.en                27.6228            N/A        28.7881        27.1564        27.4566        26.3553        23.8832        28.3550        27.7949        28.0585        25.8856        27.9259        27.4687        28.1144        26.7096        28.9676        28.3898
madar.test.mgr.0.ma.en                27.6136            N/A        28.5208        27.0323        27.2541        26.0621        24.1029        28.1778        27.4389        28.0933        25.7328        27.5366        28.7384        28.0734        26.3522        28.3746        27.9891
madar.test.mgr.0.tn.en                22.9361            N/A        23.8215        22.6493        23.0224        22.1723        20.6672        23.5120        22.8545        23.3376        21.6986        23.1674        23.6292        23.3676        22.1227        23.7454        23.4324
madar.test.mgr.1.ly.en                31.5781            N/A        32.7383        30.8096        31.2537        29.7417        27.3425        32.1286        31.1471        31.8397        29.6373        31.9388        30.6702        32.0205        29.8554        32.7550        32.4154
madar.test.mgr.1.ma.en                34.1237            N/A        34.8866        33.2697        33.3119        31.6076        28.7621        34.3709        33.3606        34.1580        30.6992        34.1349        32.3232        34.3008        31.8033        35.2084        34.4884
madar.test.mgr.1.tn.en                22.5616            N/A        23.3781        21.9918        22.5080        21.4085        20.5463        23.2478        22.5927        23.0443        21.2832        22.7735        21.7748        22.9063        21.6255        23.2885        22.8158
madar.test.msa.0.ms.en                48.8068            N/A        59.0700        51.0830        48.1571        45.0260        38.9535        51.8818        49.2318        50.9288        44.4508        49.8571        52.1336        50.7609        46.6238        52.2657        50.5144
madar.test.nil.0.eg.en                29.4470            N/A        31.6101        29.5370        29.4384        27.9810        25.3114        30.2545        29.4388        30.1942        27.8578        29.7754        30.2406        30.4932        28.4302        30.9167        30.2610
madar.test.nil.0.sd.en                37.2437            N/A        38.3622        36.3953        37.5943        35.1361        30.7697        38.2752        37.3619        37.2714        32.9454        37.6529        34.8722        37.3339        35.0884        39.3267        38.5430
madar.test.nil.1.eg.en                30.0176            N/A        31.5952        29.7526        29.7320        28.0379        25.2288        30.7062        29.7700        30.2246        27.9718        30.1105        29.2066        30.3485        28.4325        31.3839        30.6043
madar.test.nil.2.eg.en                27.5281            N/A        29.1548        27.1236        27.1722        25.9179        23.9211        28.2929        27.4099        28.2196        26.8668        27.9050        26.8927        28.0720        26.3722        28.8259        28.2178



General Arabic COMET_Unbabel_XCOMET-XL Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  0.8239            N/A         0.8329         0.7987         0.8158         0.8045         0.7759         0.8337         0.8230         0.8307         0.7831         0.8211         0.8406         0.8330         0.8060         0.8373         0.8304
QAraC.test.glf.0.qa.en                 0.8233            N/A         0.8309         0.7970         0.8135         0.8047         0.7768         0.8326         0.8221         0.8293         0.7806         0.8196         0.8399         0.8304         0.8066         0.8358         0.8283
bible.dev.mgr.0.ma.en                  0.5700            N/A         0.5558         0.5129         0.5393         0.5448         0.4523         0.5376         0.5826         0.5853         0.4421         0.5424         0.5847         0.5601         0.5004         0.5910         0.5748
bible.dev.mgr.0.tn.en                  0.6314            N/A         0.6110         0.5658         0.5863         0.5930         0.4970         0.5879         0.6348         0.6372         0.4776         0.5929         0.6359         0.6137         0.5503         0.6511         0.6349
bible.dev.msa.0.ms.en                  0.8075            N/A         0.7791         0.7268         0.7542         0.7508         0.6543         0.7519         0.8072         0.8068         0.6204         0.7553         0.8107         0.7857         0.7154         0.8268         0.8094
bible.dev.msa.1.ms.en                  0.7812            N/A         0.7699         0.6970         0.7401         0.7340         0.6249         0.7496         0.7831         0.7935         0.6309         0.7361         0.2413         0.7797         0.6874         0.8019         0.7903
bible.test.mgr.0.ma.en                 0.5553            N/A         0.5496         0.5038         0.5190         0.5268         0.4219         0.5175         0.5455         0.5588         0.4319         0.5242         0.5570         0.5451         0.4769         0.5709         0.5594
bible.test.mgr.0.tn.en                 0.6133            N/A         0.5990         0.5516         0.5736         0.5827         0.4638         0.5697         0.6077         0.6202         0.4709         0.5764         0.6135         0.5984         0.5262         0.6232         0.6196
bible.test.msa.0.ms.en                 0.8031            N/A         0.7814         0.7257         0.7508         0.7497         0.6307         0.7447         0.7954         0.8007         0.6149         0.7546         0.8001         0.7837         0.6960         0.8142         0.8039
bible.test.msa.1.ms.en                 0.7791            N/A         0.7734         0.6958         0.7385         0.7369         0.5946         0.7389         0.7722         0.7916         0.6217         0.7356         0.7870         0.7762         0.6654         0.7938         0.7857
madar.dev.glf.0.qa.en                  0.9114            N/A         0.9085         0.8902         0.9012         0.8960         0.8725         0.9180         0.9116         0.9170         0.8808         0.9096         0.9249         0.9155         0.8990         0.9194         0.9141
madar.dev.lev.0.lb.en                  0.8743            N/A         0.8696         0.8548         0.8635         0.8569         0.8291         0.8807         0.8736         0.8796         0.8431         0.8720         0.8869         0.8776         0.8605         0.8833         0.8788
madar.dev.mgr.0.ma.en                  0.7865            N/A         0.7816         0.7686         0.7776         0.7721         0.7461         0.7935         0.7873         0.7933         0.7584         0.7859         0.8001         0.7904         0.7752         0.7945         0.7890
madar.dev.mgr.0.tn.en                  0.8083            N/A         0.8060         0.7907         0.8002         0.7959         0.7695         0.8169         0.8104         0.8165         0.7791         0.8088         0.8213         0.8140         0.7983         0.8183         0.8133
madar.dev.msa.0.ms.en                  0.9405            N/A         0.9516         0.9230         0.9263         0.9211         0.8989         0.9462         0.9393         0.9448         0.9095         0.9375         0.9545         0.9438         0.9258         0.9480         0.9421
madar.dev.nil.0.eg.en                  0.8970            N/A         0.8962         0.8769         0.8845         0.8779         0.8541         0.9037         0.8952         0.9025         0.8645         0.8942         0.9127         0.8995         0.8826         0.9045         0.8988
madar.test.glf.0.iq.en                 0.9028            N/A         0.8985         0.8798         0.8894         0.8873         0.8637         0.9064         0.9026         0.9069         0.8683         0.8986         0.9139         0.9059         0.8880         0.9112         0.9051
madar.test.glf.0.om.en                 0.9294            N/A         0.9234         0.9023         0.9165         0.9124         0.8901         0.9313         0.9286         0.9314         0.8935         0.9255         0.9379         0.9285         0.9115         0.9359         0.9300
madar.test.glf.0.qa.en                 0.9135            N/A         0.9106         0.8908         0.9036         0.8982         0.8747         0.9186         0.9136         0.9178         0.8824         0.9102         0.9258         0.9179         0.9002         0.9204         0.9156
madar.test.glf.0.sa.en                 0.9391            N/A         0.9342         0.9141         0.9280         0.9233         0.9030         0.9428         0.9393         0.9420         0.9053         0.9357         0.9478         0.9394         0.9244         0.9459         0.9404
madar.test.glf.0.ye.en                 0.9110            N/A         0.9059         0.8865         0.8982         0.8945         0.8729         0.9146         0.9108         0.9136         0.8773         0.9071         0.9212         0.9127         0.8957         0.9186         0.9122
madar.test.glf.1.iq.en                 0.9027            N/A         0.8982         0.8802         0.8904         0.8847         0.8641         0.9052         0.9027         0.9047         0.8661         0.8961         0.9123         0.9046         0.8873         0.9101         0.9051
madar.test.glf.1.sa.en                 0.9142            N/A         0.9096         0.8894         0.9025         0.8980         0.8749         0.9190         0.9135         0.9174         0.8798         0.9108         0.9238         0.9167         0.8989         0.9217         0.9156
madar.test.glf.2.iq.en                 0.8929            N/A         0.8885         0.8668         0.8807         0.8763         0.8518         0.8961         0.8920         0.8957         0.8559         0.8884         0.9040         0.8947         0.8783         0.9021         0.8966
madar.test.lev.0.jo.en                 0.9146            N/A         0.9109         0.8898         0.9021         0.8983         0.8738         0.9194         0.9155         0.9188         0.8804         0.9115         0.9254         0.9161         0.9005         0.9220         0.9181
madar.test.lev.0.lb.en                 0.8674            N/A         0.8656         0.8461         0.8575         0.8513         0.8243         0.8734         0.8683         0.8732         0.8363         0.8639         0.8816         0.8725         0.8542         0.8760         0.8713
madar.test.lev.0.pa.en                 0.9078            N/A         0.9049         0.8822         0.8961         0.8936         0.8688         0.9129         0.9082         0.9126         0.8726         0.9052         0.9187         0.9107         0.8941         0.9159         0.9108
madar.test.lev.0.sy.en                 0.9070            N/A         0.9042         0.8821         0.8951         0.8902         0.8667         0.9123         0.9071         0.9124         0.8731         0.9032         0.9199         0.9104         0.8918         0.9168         0.9105
madar.test.lev.1.jo.en                 0.9115            N/A         0.9068         0.8889         0.8997         0.8959         0.8703         0.9156         0.9109         0.9153         0.8748         0.9083         0.9227         0.9133         0.8962         0.9197         0.9148
madar.test.lev.1.sy.en                 0.9003            N/A         0.8981         0.8759         0.8893         0.8847         0.8606         0.9072         0.9004         0.9055         0.8673         0.8983         0.9131         0.9043         0.8864         0.9098         0.9035
madar.test.mgr.0.dz.en                 0.8726            N/A         0.8681         0.8466         0.8601         0.8572         0.8304         0.8765         0.8725         0.8774         0.8357         0.8687         0.8853         0.8759         0.8574         0.8795         0.8744
madar.test.mgr.0.ly.en                 0.8840            N/A         0.8768         0.8574         0.8718         0.8681         0.8413         0.8877         0.8833         0.8877         0.8478         0.8799         0.8951         0.8855         0.8684         0.8908         0.8842
madar.test.mgr.0.ma.en                 0.8374            N/A         0.8329         0.8154         0.8258         0.8195         0.7906         0.8422         0.8372         0.8415         0.8054         0.8324         0.8505         0.8394         0.8228         0.8434         0.8393
madar.test.mgr.0.tn.en                 0.7979            N/A         0.7932         0.7779         0.7890         0.7837         0.7573         0.8020         0.7980         0.8021         0.7664         0.7961         0.8077         0.8000         0.7843         0.8052         0.8002
madar.test.mgr.1.ly.en                 0.8972            N/A         0.8906         0.8707         0.8856         0.8793         0.8543         0.9004         0.8962         0.9007         0.8600         0.8926         0.9071         0.8981         0.8820         0.9029         0.8977
madar.test.mgr.1.ma.en                 0.8699            N/A         0.8630         0.8459         0.8573         0.8518         0.8242         0.8737         0.8702         0.8727         0.8323         0.8644         0.8810         0.8704         0.8518         0.8782         0.8704
madar.test.mgr.1.tn.en                 0.8020            N/A         0.7948         0.7784         0.7922         0.7874         0.7631         0.8066         0.8015         0.8063         0.7664         0.7998         0.8135         0.8060         0.7876         0.8096         0.8026
madar.test.msa.0.ms.en                 0.9393            N/A         0.9479         0.9195         0.9273         0.9198         0.8969         0.9444         0.9394         0.9433         0.9100         0.9349         0.9543         0.9428         0.9242         0.9470         0.9419
madar.test.nil.0.eg.en                 0.9045            N/A         0.9044         0.8833         0.8946         0.8875         0.8630         0.9101         0.9047         0.9082         0.8734         0.9007         0.9185         0.9085         0.8911         0.9124         0.9076
madar.test.nil.0.sd.en                 0.9150            N/A         0.9094         0.8914         0.9048         0.8997         0.8782         0.9187         0.9149         0.9181         0.8800         0.9106         0.9242         0.9162         0.8990         0.9228         0.9169
madar.test.nil.1.eg.en                 0.9127            N/A         0.9117         0.8891         0.9011         0.8960         0.8731         0.9180         0.9130         0.9173         0.8787         0.9085         0.9252         0.9166         0.8980         0.9222         0.9169
madar.test.nil.2.eg.en                 0.8966            N/A         0.8970         0.8762         0.8845         0.8797         0.8567         0.9022         0.8980         0.9018         0.8635         0.8939         0.9111         0.9012         0.8827         0.9066         0.9007



General Arabic COMET_Unbabel_XCOMET-XL_ref-less Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  0.9218            N/A         0.9204         0.8924         0.9097         0.9031         0.8899         0.9244         0.9211         0.9226         0.8753         0.9167         0.9312         0.9226         0.9076         0.9280         0.9248
QAraC.test.glf.0.qa.en                 0.9216            N/A         0.9203         0.8904         0.9088         0.9052         0.8900         0.9252         0.9215         0.9223         0.8742         0.9173         0.9323         0.9219         0.9086         0.9283         0.9240
bible.dev.mgr.0.ma.en                  0.8734            N/A         0.7994         0.7862         0.7950         0.8035         0.7456         0.7635         0.8649         0.8537         0.5939         0.8066         0.8275         0.8034         0.7882         0.8728         0.8634
bible.dev.mgr.0.tn.en                  0.8750            N/A         0.8018         0.7864         0.7954         0.8032         0.7435         0.7642         0.8649         0.8538         0.5935         0.8074         0.8280         0.8042         0.7866         0.8701         0.8649
bible.dev.msa.0.ms.en                  0.8732            N/A         0.8012         0.7890         0.7929         0.8039         0.7432         0.7641         0.8651         0.8538         0.5935         0.8066         0.8280         0.8052         0.7857         0.8713         0.8647
bible.dev.msa.1.ms.en                  0.8741            N/A         0.8000         0.7849         0.7949         0.8036         0.7453         0.7637         0.8648         0.8540         0.5941         0.8046         0.2455         0.8037         0.7857         0.8688         0.8640
bible.test.mgr.0.ma.en                 0.8662            N/A         0.8108         0.7828         0.7985         0.8085         0.7291         0.7603         0.8479         0.8472         0.6008         0.8069         0.8187         0.8144         0.7717         0.8579         0.8565
bible.test.mgr.0.tn.en                 0.8675            N/A         0.8132         0.7783         0.7984         0.8084         0.7309         0.7602         0.8479         0.8473         0.6020         0.8068         0.8181         0.8141         0.7695         0.8555         0.8586
bible.test.msa.0.ms.en                 0.8658            N/A         0.8134         0.7839         0.7984         0.8089         0.7308         0.7601         0.8481         0.8472         0.6014         0.8079         0.8151         0.8140         0.7714         0.8577         0.8560
bible.test.msa.1.ms.en                 0.8668            N/A         0.8123         0.7809         0.8002         0.8095         0.7294         0.7592         0.8483         0.8473         0.6014         0.8070         0.8236         0.8163         0.7709         0.8569         0.8557
madar.dev.glf.0.qa.en                  0.9472            N/A         0.9376         0.9211         0.9349         0.9316         0.9215         0.9483         0.9449         0.9476         0.9141         0.9440         0.9567         0.9461         0.9361         0.9489         0.9458
madar.dev.lev.0.lb.en                  0.9473            N/A         0.9377         0.9216         0.9351         0.9313         0.9216         0.9483         0.9449         0.9476         0.9142         0.9440         0.9568         0.9460         0.9356         0.9485         0.9458
madar.dev.mgr.0.ma.en                  0.9472            N/A         0.9375         0.9215         0.9348         0.9316         0.9215         0.9483         0.9449         0.9475         0.9141         0.9438         0.9565         0.9460         0.9357         0.9486         0.9456
madar.dev.mgr.0.tn.en                  0.9473            N/A         0.9376         0.9210         0.9354         0.9316         0.9219         0.9483         0.9449         0.9476         0.9141         0.9440         0.9568         0.9461         0.9358         0.9488         0.9462
madar.dev.msa.0.ms.en                  0.9472            N/A         0.9375         0.9215         0.9349         0.9318         0.9217         0.9483         0.9449         0.9476         0.9141         0.9440         0.9568         0.9460         0.9360         0.9488         0.9458
madar.dev.nil.0.eg.en                  0.9472            N/A         0.9377         0.9216         0.9350         0.9316         0.9214         0.9483         0.9449         0.9476         0.9141         0.9439         0.9567         0.9461         0.9358         0.9489         0.9458
madar.test.glf.0.iq.en                 0.9461            N/A         0.9369         0.9183         0.9350         0.9327         0.9230         0.9483         0.9460         0.9480         0.9135         0.9424         0.9547         0.9470         0.9342         0.9495         0.9448
madar.test.glf.0.om.en                 0.9459            N/A         0.9373         0.9179         0.9351         0.9328         0.9219         0.9481         0.9460         0.9480         0.9134         0.9422         0.9541         0.9471         0.9344         0.9490         0.9449
madar.test.glf.0.qa.en                 0.9473            N/A         0.9386         0.9209         0.9358         0.9330         0.9212         0.9483         0.9463         0.9472         0.9166         0.9429         0.9558         0.9468         0.9360         0.9485         0.9459
madar.test.glf.0.sa.en                 0.9456            N/A         0.9367         0.9187         0.9351         0.9330         0.9218         0.9481         0.9460         0.9481         0.9136         0.9423         0.9543         0.9469         0.9341         0.9493         0.9454
madar.test.glf.0.ye.en                 0.9464            N/A         0.9375         0.9190         0.9353         0.9328         0.9221         0.9482         0.9460         0.9480         0.9132         0.9422         0.9545         0.9470         0.9342         0.9490         0.9451
madar.test.glf.1.iq.en                 0.9466            N/A         0.9371         0.9205         0.9351         0.9326         0.9221         0.9482         0.9460         0.9480         0.9132         0.9421         0.9545         0.9470         0.9349         0.9492         0.9448
madar.test.glf.1.sa.en                 0.9460            N/A         0.9372         0.9187         0.9354         0.9326         0.9216         0.9483         0.9460         0.9480         0.9133         0.9422         0.9535         0.9472         0.9347         0.9497         0.9450
madar.test.glf.2.iq.en                 0.9465            N/A         0.9374         0.9166         0.9359         0.9333         0.9222         0.9482         0.9459         0.9480         0.9136         0.9422         0.9547         0.9470         0.9342         0.9490         0.9452
madar.test.lev.0.jo.en                 0.9465            N/A         0.9374         0.9184         0.9355         0.9330         0.9216         0.9481         0.9459         0.9480         0.9133         0.9421         0.9540         0.9469         0.9348         0.9491         0.9451
madar.test.lev.0.lb.en                 0.9473            N/A         0.9387         0.9197         0.9358         0.9328         0.9209         0.9483         0.9463         0.9473         0.9166         0.9429         0.9560         0.9469         0.9361         0.9486         0.9462
madar.test.lev.0.pa.en                 0.9462            N/A         0.9374         0.9176         0.9352         0.9327         0.9216         0.9481         0.9460         0.9480         0.9132         0.9421         0.9538         0.9472         0.9336         0.9490         0.9444
madar.test.lev.0.sy.en                 0.9463            N/A         0.9378         0.9178         0.9357         0.9329         0.9213         0.9482         0.9460         0.9480         0.9132         0.9421         0.9538         0.9473         0.9340         0.9496         0.9456
madar.test.lev.1.jo.en                 0.9462            N/A         0.9373         0.9210         0.9350         0.9331         0.9215         0.9480         0.9460         0.9480         0.9133         0.9422         0.9544         0.9471         0.9347         0.9492         0.9451
madar.test.lev.1.sy.en                 0.9463            N/A         0.9378         0.9173         0.9355         0.9333         0.9220         0.9483         0.9459         0.9479         0.9131         0.9421         0.9543         0.9470         0.9340         0.9492         0.9447
madar.test.mgr.0.dz.en                 0.9464            N/A         0.9370         0.9184         0.9356         0.9324         0.9219         0.9482         0.9460         0.9480         0.9134         0.9421         0.9543         0.9472         0.9347         0.9494         0.9446
madar.test.mgr.0.ly.en                 0.9464            N/A         0.9372         0.9184         0.9353         0.9327         0.9210         0.9482         0.9460         0.9480         0.9131         0.9420         0.9536         0.9470         0.9343         0.9493         0.9449
madar.test.mgr.0.ma.en                 0.9474            N/A         0.9385         0.9209         0.9357         0.9331         0.9213         0.9482         0.9463         0.9471         0.9165         0.9429         0.9563         0.9467         0.9361         0.9487         0.9461
madar.test.mgr.0.tn.en                 0.9473            N/A         0.9385         0.9201         0.9356         0.9327         0.9212         0.9482         0.9464         0.9472         0.9165         0.9427         0.9557         0.9468         0.9359         0.9487         0.9462
madar.test.mgr.1.ly.en                 0.9462            N/A         0.9371         0.9171         0.9351         0.9330         0.9220         0.9482         0.9460         0.9480         0.9132         0.9424         0.9541         0.9473         0.9339         0.9491         0.9446
madar.test.mgr.1.ma.en                 0.9460            N/A         0.9370         0.9194         0.9348         0.9329         0.9226         0.9482         0.9459         0.9480         0.9135         0.9422         0.9540         0.9471         0.9337         0.9493         0.9445
madar.test.mgr.1.tn.en                 0.9461            N/A         0.9369         0.9182         0.9356         0.9325         0.9214         0.9482         0.9460         0.9480         0.9134         0.9420         0.9542         0.9470         0.9346         0.9491         0.9447
madar.test.msa.0.ms.en                 0.9474            N/A         0.9387         0.9206         0.9359         0.9328         0.9209         0.9482         0.9464         0.9472         0.9166         0.9428         0.9558         0.9466         0.9358         0.9485         0.9462
madar.test.nil.0.eg.en                 0.9474            N/A         0.9384         0.9211         0.9357         0.9327         0.9213         0.9482         0.9463         0.9472         0.9166         0.9428         0.9557         0.9469         0.9361         0.9484         0.9462
madar.test.nil.0.sd.en                 0.9460            N/A         0.9371         0.9193         0.9347         0.9329         0.9213         0.9482         0.9460         0.9480         0.9132         0.9420         0.9546         0.9472         0.9338         0.9490         0.9453
madar.test.nil.1.eg.en                 0.9465            N/A         0.9377         0.9173         0.9351         0.9326         0.9213         0.9481         0.9460         0.9480         0.9132         0.9421         0.9544         0.9472         0.9342         0.9489         0.9450
madar.test.nil.2.eg.en                 0.9462            N/A         0.9374         0.9187         0.9352         0.9327         0.9218         0.9482         0.9460         0.9481         0.9135         0.9423         0.9545         0.9471         0.9341         0.9495         0.9451


================================================================================
 Average BLEU Scores for General Arabic Translations
================================================================================

Models sorted by average BLEU score:

Model: Jais-2-70B-Chat
  Average BLEU: 5.8256
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average BLEU: 5.5714
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average BLEU: 5.3019
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average BLEU: 5.2853
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average BLEU: 5.2103
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average BLEU: 5.2085
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average BLEU: 5.1923
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average BLEU: 4.9284
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average BLEU: 4.8970
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average BLEU: 4.8843
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average BLEU: 4.4276
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average BLEU: 4.3968
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average BLEU: 4.0545
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average BLEU: 4.0383
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average BLEU: 3.8831
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average BLEU: 3.1585
  Based on 42 test set(s)

================================================================================
 Average CHRF Scores for General Arabic Translations
================================================================================

Models sorted by average CHRF score:

Model: Jais-2-70B-Chat
  Average CHRF: 32.6186
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average CHRF: 32.3026
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average CHRF: 31.6765
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average CHRF: 31.5539
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average CHRF: 31.5299
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average CHRF: 31.1730
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average CHRF: 30.9127
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average CHRF: 30.8236
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average CHRF: 30.6644
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average CHRF: 30.4584
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average CHRF: 30.0612
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average CHRF: 29.8604
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average CHRF: 29.2102
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average CHRF: 28.6911
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average CHRF: 27.7208
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average CHRF: 26.3114
  Based on 42 test set(s)

================================================================================
 Average COMET_Unbabel_XCOMET-XL Scores for General Arabic Translations
================================================================================

Models sorted by average COMET_Unbabel_XCOMET-XL score:

Model: gpt-4.1-mini
  Average COMET_Unbabel_XCOMET-XL: 0.8586
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average COMET_Unbabel_XCOMET-XL: 0.8536
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average COMET_Unbabel_XCOMET-XL: 0.8518
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average COMET_Unbabel_XCOMET-XL: 0.8489
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average COMET_Unbabel_XCOMET-XL: 0.8488
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average COMET_Unbabel_XCOMET-XL: 0.8484
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average COMET_Unbabel_XCOMET-XL: 0.8465
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average COMET_Unbabel_XCOMET-XL: 0.8448
  Based on 42 test set(s)

Model: Jais-2-70B-Chat
  Average COMET_Unbabel_XCOMET-XL: 0.8439
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average COMET_Unbabel_XCOMET-XL: 0.8384
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average COMET_Unbabel_XCOMET-XL: 0.8316
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average COMET_Unbabel_XCOMET-XL: 0.8278
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average COMET_Unbabel_XCOMET-XL: 0.8197
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average COMET_Unbabel_XCOMET-XL: 0.8168
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average COMET_Unbabel_XCOMET-XL: 0.7920
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average COMET_Unbabel_XCOMET-XL: 0.7869
  Based on 42 test set(s)

================================================================================
 Average COMET_Unbabel_XCOMET-XL_ref-less Scores for General Arabic Translations
================================================================================

Models sorted by average COMET_Unbabel_XCOMET-XL_ref-less score:

Model: gpt-4.1-mini
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9318
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9309
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9282
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9280
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9277
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9195
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9155
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9150
  Based on 42 test set(s)

Model: Jais-2-70B-Chat
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9118
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9116
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9077
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9072
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9039
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8923
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8850
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8519
  Based on 42 test set(s)

================================================================================
 Ranking Statistics for General Arabic BLEU Score
================================================================================
Total test sets with General Arabic BLEU scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  1st: 1 time
  3rd: 2 times
  4th: 2 times
  5th: 2 times
  6th: 1 time
  7th: 7 times
  8th: 6 times
  9th: 6 times
  10th: 10 times
  11th: 4 times
  13th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 3/42 (7.1%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 9 times
  2nd: 3 times
  3rd: 4 times
  4th: 1 time
  5th: 6 times
  6th: 1 time
  7th: 2 times
  8th: 6 times
  9th: 4 times
  10th: 5 times
  11th: 1 time
  Win rate: 9/42 (21.4%)
  Top-3 rate: 16/42 (38.1%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  2nd: 2 times
  10th: 2 times
  11th: 15 times
  12th: 7 times
  13th: 10 times
  14th: 4 times
  15th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  3rd: 2 times
  4th: 3 times
  5th: 2 times
  6th: 4 times
  7th: 1 time
  8th: 9 times
  9th: 8 times
  10th: 7 times
  11th: 4 times
  12th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  8th: 1 time
  11th: 2 times
  12th: 13 times
  13th: 13 times
  14th: 9 times
  15th: 3 times
  16th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  13th: 1 time
  15th: 5 times
  16th: 36 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  1st: 1 time
  2nd: 1 time
  3rd: 3 times
  4th: 8 times
  5th: 6 times
  6th: 10 times
  7th: 11 times
  9th: 1 time
  10th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 5/42 (11.9%)

Model: aya-expanse-8b
  Total appearances: 42/42
  2nd: 2 times
  5th: 3 times
  6th: 1 time
  7th: 7 times
  8th: 5 times
  9th: 8 times
  10th: 11 times
  11th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 2 times
  3rd: 6 times
  4th: 6 times
  5th: 6 times
  6th: 6 times
  7th: 7 times
  8th: 6 times
  9th: 2 times
  10th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 8/42 (19.0%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  4th: 1 time
  9th: 2 times
  11th: 5 times
  12th: 5 times
  13th: 2 times
  14th: 9 times
  15th: 14 times
  16th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  1st: 2 times
  2nd: 10 times
  3rd: 10 times
  4th: 3 times
  5th: 4 times
  6th: 1 time
  7th: 2 times
  8th: 2 times
  9th: 2 times
  10th: 1 time
  11th: 2 times
  12th: 2 times
  14th: 1 time
  Win rate: 2/42 (4.8%)
  Top-3 rate: 22/42 (52.4%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  3rd: 2 times
  4th: 3 times
  5th: 1 time
  6th: 1 time
  7th: 1 time
  9th: 4 times
  10th: 1 time
  11th: 4 times
  12th: 5 times
  13th: 5 times
  14th: 5 times
  15th: 9 times
  16th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  1st: 5 times
  2nd: 4 times
  3rd: 2 times
  4th: 8 times
  5th: 6 times
  6th: 6 times
  7th: 1 time
  8th: 4 times
  9th: 5 times
  10th: 1 time
  Win rate: 5/42 (11.9%)
  Top-3 rate: 11/42 (26.2%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  10th: 1 time
  12th: 8 times
  13th: 10 times
  14th: 14 times
  15th: 9 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 21 times
  2nd: 12 times
  3rd: 3 times
  4th: 2 times
  5th: 1 time
  6th: 3 times
  Win rate: 21/42 (50.0%)
  Top-3 rate: 36/42 (85.7%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 3 times
  2nd: 6 times
  3rd: 8 times
  4th: 5 times
  5th: 5 times
  6th: 8 times
  7th: 3 times
  8th: 3 times
  10th: 1 time
  Win rate: 3/42 (7.1%)
  Top-3 rate: 17/42 (40.5%)

================================================================================
 Ranking Statistics for General Arabic CHRF Score
================================================================================
Total test sets with General Arabic CHRF scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  4th: 3 times
  5th: 2 times
  6th: 5 times
  7th: 3 times
  8th: 12 times
  9th: 6 times
  10th: 7 times
  11th: 3 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 19 times
  2nd: 12 times
  3rd: 5 times
  4th: 4 times
  6th: 2 times
  Win rate: 19/42 (45.2%)
  Top-3 rate: 36/42 (85.7%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  3rd: 1 time
  5th: 1 time
  9th: 2 times
  10th: 1 time
  11th: 18 times
  12th: 10 times
  13th: 2 times
  14th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  6th: 1 time
  7th: 1 time
  8th: 5 times
  9th: 18 times
  10th: 11 times
  11th: 3 times
  12th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  11th: 1 time
  12th: 4 times
  13th: 7 times
  14th: 16 times
  15th: 14 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  14th: 1 time
  15th: 7 times
  16th: 34 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  2nd: 1 time
  3rd: 8 times
  4th: 20 times
  5th: 2 times
  6th: 2 times
  7th: 1 time
  8th: 1 time
  11th: 2 times
  12th: 4 times
  13th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 9/42 (21.4%)

Model: aya-expanse-8b
  Total appearances: 42/42
  5th: 1 time
  6th: 3 times
  7th: 5 times
  8th: 2 times
  9th: 8 times
  10th: 12 times
  11th: 11 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  1st: 1 time
  2nd: 2 times
  3rd: 5 times
  4th: 4 times
  5th: 7 times
  6th: 11 times
  7th: 7 times
  8th: 4 times
  9th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 8/42 (19.0%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  13th: 6 times
  14th: 10 times
  15th: 19 times
  16th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  5th: 3 times
  6th: 5 times
  7th: 13 times
  8th: 10 times
  9th: 6 times
  10th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 1 time
  2nd: 1 time
  3rd: 4 times
  5th: 4 times
  6th: 1 time
  7th: 2 times
  8th: 4 times
  9th: 1 time
  10th: 2 times
  11th: 3 times
  12th: 15 times
  14th: 1 time
  15th: 2 times
  16th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 6/42 (14.3%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  2nd: 2 times
  3rd: 4 times
  4th: 3 times
  5th: 17 times
  6th: 9 times
  7th: 5 times
  8th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  10th: 3 times
  11th: 1 time
  12th: 5 times
  13th: 26 times
  14th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 21 times
  2nd: 17 times
  3rd: 2 times
  4th: 2 times
  Win rate: 21/42 (50.0%)
  Top-3 rate: 40/42 (95.2%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  2nd: 7 times
  3rd: 13 times
  4th: 6 times
  5th: 5 times
  6th: 4 times
  7th: 4 times
  8th: 2 times
  10th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 20/42 (47.6%)

================================================================================
 Ranking Statistics for General Arabic COMET_Unbabel_XCOMET-XL Score
================================================================================
Total test sets with General Arabic COMET_Unbabel_XCOMET-XL scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  3rd: 1 time
  4th: 1 time
  5th: 4 times
  6th: 3 times
  7th: 13 times
  8th: 17 times
  9th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  2nd: 2 times
  4th: 1 time
  5th: 1 time
  6th: 1 time
  7th: 3 times
  8th: 6 times
  9th: 8 times
  10th: 20 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  12th: 1 time
  13th: 8 times
  14th: 33 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  9th: 1 time
  10th: 3 times
  11th: 37 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  9th: 4 times
  11th: 3 times
  12th: 4 times
  13th: 30 times
  14th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  15th: 5 times
  16th: 37 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  3rd: 24 times
  4th: 10 times
  8th: 1 time
  9th: 1 time
  11th: 2 times
  12th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 24/42 (57.1%)

Model: aya-expanse-8b
  Total appearances: 42/42
  4th: 2 times
  5th: 2 times
  6th: 2 times
  7th: 18 times
  8th: 14 times
  9th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 5 times
  3rd: 8 times
  4th: 20 times
  5th: 6 times
  6th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 13/42 (31.0%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  14th: 1 time
  15th: 37 times
  16th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  8th: 1 time
  9th: 21 times
  10th: 19 times
  12th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 34 times
  2nd: 1 time
  3rd: 3 times
  4th: 2 times
  5th: 1 time
  16th: 1 time
  Win rate: 34/42 (81.0%)
  Top-3 rate: 38/42 (90.5%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  4th: 3 times
  5th: 16 times
  6th: 16 times
  7th: 4 times
  8th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  12th: 31 times
  13th: 4 times
  14th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 8 times
  2nd: 32 times
  3rd: 2 times
  Win rate: 8/42 (19.0%)
  Top-3 rate: 42/42 (100.0%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  2nd: 2 times
  3rd: 4 times
  4th: 3 times
  5th: 12 times
  6th: 17 times
  7th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

================================================================================
 Ranking Statistics for General Arabic COMET_Unbabel_XCOMET-XL_ref-less Score
================================================================================
Total test sets with General Arabic COMET_Unbabel_XCOMET-XL_ref-less scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  1st: 8 times
  4th: 6 times
  5th: 6 times
  6th: 17 times
  7th: 5 times
  Win rate: 8/42 (19.0%)
  Top-3 rate: 8/42 (19.0%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  8th: 4 times
  9th: 3 times
  10th: 35 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  12th: 6 times
  13th: 2 times
  14th: 5 times
  15th: 29 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  10th: 1 time
  11th: 30 times
  12th: 11 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  8th: 2 times
  9th: 6 times
  13th: 34 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  14th: 30 times
  15th: 12 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  3rd: 33 times
  4th: 1 time
  13th: 1 time
  14th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 33/42 (78.6%)

Model: aya-expanse-8b
  Total appearances: 42/42
  3rd: 3 times
  4th: 5 times
  6th: 3 times
  7th: 23 times
  8th: 8 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 3/42 (7.1%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  4th: 26 times
  5th: 15 times
  6th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  15th: 1 time
  16th: 41 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  6th: 1 time
  7th: 3 times
  9th: 32 times
  10th: 6 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 34 times
  6th: 7 times
  16th: 1 time
  Win rate: 34/42 (81.0%)
  Top-3 rate: 34/42 (81.0%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  5th: 21 times
  6th: 12 times
  7th: 6 times
  8th: 2 times
  9th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  11th: 12 times
  12th: 25 times
  13th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  2nd: 41 times
  3rd: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 42/42 (100.0%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  2nd: 1 time
  3rd: 5 times
  4th: 4 times
  6th: 1 time
  7th: 5 times
  8th: 26 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

================================================================================
 Average Rankings for General Arabic BLEU Score
================================================================================
Lower rank is better (1st = best)

1. gpt-4.1-mini: 2.02 (based on 42 test set(s))
2. gpt-4.1-nano: 4.45 (based on 42 test set(s))
3. c4ai-command-r7b-arabic-02-2025: 4.95 (based on 42 test set(s))
4. gemma-3-27b-it: 5.02 (based on 42 test set(s))
5. Jais-2-70B-Chat: 5.38 (based on 42 test set(s))
6. aya-expanse-32b: 5.48 (based on 42 test set(s))
7. c4ai-command-r-08-2024: 5.64 (based on 42 test set(s))
8. EuroLLM-9B-Instruct: 8.07 (based on 42 test set(s))
9. Llama-3.3-70B-Instruct: 8.12 (based on 42 test set(s))
10. aya-expanse-8b: 8.36 (based on 42 test set(s))
11. command-a-translate-08-2025: 11.24 (based on 42 test set(s))
12. Jais-2-8B-Chat: 11.64 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 12.90 (based on 42 test set(s))
14. c4ai-command-r-v01: 13.40 (based on 42 test set(s))
15. gemma-3-4b-it: 13.50 (based on 42 test set(s))
16. Qwen3-4B-Instruct-2507: 15.81 (based on 42 test set(s))


================================================================================
 Average Rankings for General Arabic CHRF Score
================================================================================
Lower rank is better (1st = best)

1. gpt-4.1-mini: 1.64 (based on 42 test set(s))
2. Jais-2-70B-Chat: 2.05 (based on 42 test set(s))
3. gpt-4.1-nano: 4.29 (based on 42 test set(s))
4. gemma-3-27b-it: 5.20 (based on 42 test set(s))
5. aya-expanse-32b: 5.38 (based on 42 test set(s))
6. c4ai-command-r-08-2024: 5.40 (based on 42 test set(s))
7. c4ai-command-r7b-arabic-02-2025: 7.63 (based on 42 test set(s))
8. EuroLLM-9B-Instruct: 8.05 (based on 42 test set(s))
9. aya-expanse-8b: 9.21 (based on 42 test set(s))
10. command-a-translate-08-2025: 9.26 (based on 42 test set(s))
11. Llama-3.3-70B-Instruct: 9.38 (based on 42 test set(s))
12. Jais-2-8B-Chat: 11.38 (based on 42 test set(s))
13. gemma-3-4b-it: 12.79 (based on 42 test set(s))
14. Mistral-Small-3.2-24B-Instruct-2506: 13.90 (based on 42 test set(s))
15. c4ai-command-r-v01: 14.64 (based on 42 test set(s))
16. Qwen3-4B-Instruct-2507: 15.79 (based on 42 test set(s))


================================================================================
 Average Rankings for General Arabic COMET_Unbabel_XCOMET-XL Score
================================================================================
Lower rank is better (1st = best)

1. command-a-translate-08-2025: 1.76 (based on 42 test set(s))
2. gpt-4.1-mini: 1.86 (based on 42 test set(s))
3. c4ai-command-r-08-2024: 3.86 (based on 42 test set(s))
4. aya-expanse-32b: 4.74 (based on 42 test set(s))
5. gpt-4.1-nano: 5.19 (based on 42 test set(s))
6. gemma-3-27b-it: 5.71 (based on 42 test set(s))
7. EuroLLM-9B-Instruct: 7.12 (based on 42 test set(s))
8. aya-expanse-8b: 7.24 (based on 42 test set(s))
9. Jais-2-70B-Chat: 8.57 (based on 42 test set(s))
10. c4ai-command-r7b-arabic-02-2025: 9.50 (based on 42 test set(s))
11. Llama-3.3-70B-Instruct: 10.90 (based on 42 test set(s))
12. Mistral-Small-3.2-24B-Instruct-2506: 12.40 (based on 42 test set(s))
13. gemma-3-4b-it: 12.43 (based on 42 test set(s))
14. Jais-2-8B-Chat: 13.76 (based on 42 test set(s))
15. c4ai-command-r-v01: 15.07 (based on 42 test set(s))
16. Qwen3-4B-Instruct-2507: 15.88 (based on 42 test set(s))


================================================================================
 Average Rankings for General Arabic COMET_Unbabel_XCOMET-XL_ref-less Score
================================================================================
Lower rank is better (1st = best)

1. gpt-4.1-mini: 2.02 (based on 42 test set(s))
2. command-a-translate-08-2025: 2.19 (based on 42 test set(s))
3. c4ai-command-r-08-2024: 4.40 (based on 42 test set(s))
4. EuroLLM-9B-Instruct: 4.74 (based on 42 test set(s))
5. aya-expanse-32b: 5.10 (based on 42 test set(s))
6. gemma-3-27b-it: 5.81 (based on 42 test set(s))
7. aya-expanse-8b: 6.48 (based on 42 test set(s))
8. gpt-4.1-nano: 6.71 (based on 42 test set(s))
9. c4ai-command-r7b-arabic-02-2025: 8.93 (based on 42 test set(s))
10. Jais-2-70B-Chat: 9.74 (based on 42 test set(s))
11. Llama-3.3-70B-Instruct: 11.24 (based on 42 test set(s))
12. gemma-3-4b-it: 11.83 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 12.19 (based on 42 test set(s))
14. Qwen3-4B-Instruct-2507: 14.29 (based on 42 test set(s))
15. Jais-2-8B-Chat: 14.36 (based on 42 test set(s))
16. c4ai-command-r-v01: 15.98 (based on 42 test set(s))


================================================================================
 Overall Scores (All Test Sets Concatenated) - General Arabic
================================================================================

Model: EuroLLM-9B-Instruct
  BLEU: 5.3984
  CHRF: 31.0540
  Test sets: 42
  Sentences: 130226

Model: Jais-2-70B-Chat
  BLEU: 7.2703
  CHRF: 33.3178
  Test sets: 42
  Sentences: 130226

Model: Jais-2-8B-Chat
  BLEU: 5.0393
  CHRF: 30.4550
  Test sets: 42
  Sentences: 130226

Model: Llama-3.3-70B-Instruct
  BLEU: 5.4629
  CHRF: 30.8064
  Test sets: 42
  Sentences: 130226

Model: Mistral-Small-3.2-24B-Instruct-2506
  BLEU: 4.3495
  CHRF: 28.8878
  Test sets: 42
  Sentences: 130226

Model: Qwen3-4B-Instruct-2507
  BLEU: 3.5676
  CHRF: 26.5324
  Test sets: 42
  Sentences: 130226

Model: aya-expanse-32b
  BLEU: 5.9068
  CHRF: 31.7123
  Test sets: 42
  Sentences: 130226

Model: aya-expanse-8b
  BLEU: 5.4857
  CHRF: 30.9069
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r-08-2024
  BLEU: 5.8277
  CHRF: 31.7877
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r-v01
  BLEU: 4.4480
  CHRF: 28.3658
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r7b-arabic-02-2025
  BLEU: 5.8838
  CHRF: 31.1816
  Test sets: 42
  Sentences: 130226

Model: command-a-translate-08-2025
  BLEU: 5.1332
  CHRF: 31.0741
  Test sets: 42
  Sentences: 130226

Model: gemma-3-27b-it
  BLEU: 5.9046
  CHRF: 31.8192
  Test sets: 42
  Sentences: 130226

Model: gemma-3-4b-it
  BLEU: 4.6507
  CHRF: 29.5952
  Test sets: 42
  Sentences: 130226

Model: gpt-4.1-mini
  BLEU: 6.2541
  CHRF: 32.5178
  Test sets: 42
  Sentences: 130226

Model: gpt-4.1-nano
  BLEU: 5.8921
  CHRF: 31.8071
  Test sets: 42
  Sentences: 130226
