================================================================================
Statistics for Dialect Translations
Direction: Forward (English -> Arabic)
================================================================================


================================================================================
SCORE TABLES
================================================================================


Dialect BLEU Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  2.1815            N/A         5.0942         1.5345         2.7324         2.0677         1.4940         2.9279         2.2316         2.7189         1.8152         2.7360         2.6378         2.2661         1.6298         3.3040         2.8376
QAraC.test.glf.0.qa.en                 2.4113            N/A         4.9475         1.7294         3.0438         2.1156         1.6324         3.2969         2.3855         3.1085         2.0205         2.9831         2.8177         2.6057         1.8963         3.5842         3.3327
bible.dev.mgr.0.ma.en                  0.7310            N/A         1.6550         0.6458         0.8981         0.1929         0.4409         0.8432         0.6210         1.9318         0.3693         0.5711         1.1151         1.7158         0.7839         2.9038         1.4651
bible.dev.mgr.0.tn.en                  1.0485            N/A         3.0879         1.8273         1.3474         0.2672         0.9993         2.0914         1.1215         2.3835         0.5272         0.9892         2.0362         2.2688         0.9189         3.1111         2.4485
bible.dev.msa.0.ms.en                  7.2362            N/A         6.9030         4.0261         6.3549         4.4950         3.3784         6.1072         6.6624         7.3470         4.1141         6.4472         8.3279         8.6613         4.8787         8.1084         7.8676
bible.dev.msa.1.ms.en                  2.8274            N/A         5.8013         1.7225         2.9232         2.1451         1.4762         4.0980         2.9478         3.8728         2.5745         3.2617         3.9659         4.6392         1.7601         3.8737         3.5543
bible.test.mgr.0.ma.en                 0.5134            N/A         1.8408         0.6434         0.9150         0.1121         0.2057         0.6255         0.4037         1.7960         0.5235         0.3098         1.1223         1.3380         0.4194         2.7317         2.0906
bible.test.mgr.0.tn.en                 0.7237            N/A         2.3494         1.4627         0.9548         0.4005         0.7685         1.6890         1.2886         2.0061         0.9052         1.0136         2.0611         1.9384         0.8093         3.0637         2.3891
bible.test.msa.0.ms.en                 6.4163            N/A         6.5742         3.9797         5.3667         4.5092         3.3782         5.0527         6.2401         7.4536         3.8251         5.3438         7.4739         7.3720         3.8150         7.0594         7.0272
bible.test.msa.1.ms.en                 2.3063            N/A         4.3726         1.2354         2.5856         2.0158         0.9611         3.3031         2.2688         2.7930         2.4163         2.5898         3.2570         3.9017         0.9871         2.7134         2.8129
madar.dev.glf.0.qa.en                  5.1810            N/A        17.7475         7.0668         6.2664         3.7005         3.5551         8.2439         4.7353        12.3651         3.9816         5.4853         5.0309         6.4567         6.1584        15.9816         8.3809
madar.dev.lev.0.lb.en                  3.0514            N/A        42.2421        18.4803         5.8236         1.7834         2.0059         6.6448         2.5703         9.1477         2.4630         3.2402         8.0423         9.1806         4.5884        13.0817        11.3232
madar.dev.mgr.0.ma.en                  2.0715            N/A        29.4362         8.5675         3.6791         0.5774         1.1930         4.2430         2.2055         5.6634         1.8894         3.0737         7.9716         5.9402         2.4177         6.3696         6.0101
madar.dev.mgr.0.tn.en                  2.1589            N/A        47.0646        20.2660         3.8042         0.4852         1.6674         4.4399         2.4310         4.8492         2.0851         2.7084         6.3262         4.6987         2.2000         9.2611         7.5396
madar.dev.msa.0.ms.en                 17.2223            N/A        28.9647        19.6181        17.0551        15.6213        11.4146        19.6500        17.4377        18.4723        13.9224        18.4239        19.9472        18.6566        15.7108        20.2427        18.6138
madar.dev.nil.0.eg.en                  4.3210            N/A        51.9239        19.0743         8.1604         2.4583         2.3889        11.0849         4.2230        10.6362         3.1650         8.5337         4.3818        11.5263         6.5311         1.9359         5.9901
madar.test.glf.0.iq.en                 4.1470            N/A        12.2542         2.3505         5.3948         4.3470         3.0068         6.7086         4.4109         8.1553         3.1572         5.3310         5.1993         9.3502         4.6436        12.2225         9.4985
madar.test.glf.0.om.en                17.6852            N/A        10.5081         3.3012        17.3093        11.8633         7.8031        16.6327        17.8516        13.3197        13.0614        18.9940        13.6715        18.3651        14.8705        16.6275        17.0885
madar.test.glf.0.qa.en                 5.3119            N/A        17.6625         7.6189         6.6047         4.2143         4.0463         8.9295         5.2852        12.5632         4.5171         6.1218         5.6445         6.7620         6.4161        16.4629         9.0667
madar.test.glf.0.sa.en                 8.0879            N/A        17.4234         7.9447         8.4926         8.2850         6.4894         8.8046         8.4513        10.3475         7.7237        10.1750         7.7771        10.1567         9.3025        15.7981         5.3250
madar.test.glf.0.ye.en                 5.1929            N/A        10.7836         2.3786         5.9050         4.6889         3.3257         5.3645         4.4760         6.7438         3.9515         5.7380         4.5888         6.9280         5.1450        10.1072         8.7228
madar.test.glf.1.iq.en                 3.7395            N/A        10.8618         2.1542         4.5681         3.7873         2.3170         5.1338         3.3951         7.2039         2.6328         4.5189         3.7601         7.9970         3.4288        10.8540         8.0837
madar.test.glf.1.sa.en                 4.2538            N/A        13.2714         5.9823         4.4811         4.4830         3.3026         4.9063         4.6263         5.5144         4.1547         5.4566         3.9974         5.3429         5.3828         8.6030         2.9799
madar.test.glf.2.iq.en                 3.0014            N/A         5.9788         1.2583         3.7044         3.4010         2.5148         4.2080         3.1876         4.8156         2.2950         3.8246         2.9827         5.4395         3.1006         6.3458         5.5010
madar.test.lev.0.jo.en                 4.6204            N/A        16.4067        11.1646         9.1210         5.2265         3.1422        10.4078         4.8492        15.5858         4.6407         6.2058         0.0588        14.5330         7.1256        19.8333        15.3103
madar.test.lev.0.lb.en                 2.9420            N/A        35.3296        16.5318         4.9437         1.6277         2.0253         6.3015         2.7663         8.1984         2.5679         3.2124         8.1888         8.1450         3.7923        12.0381         4.9080
madar.test.lev.0.pa.en                 4.5785            N/A        17.7403         7.3225         9.3946         4.5961         2.9241         7.1838         4.8108        14.0396         4.1322         6.1669         5.8711         9.3070         5.9834        19.6689        16.1900
madar.test.lev.0.sy.en                 3.8501            N/A        18.3381        14.5109         6.3358         4.3017         2.6133         4.9672         4.0363         8.2252         3.6228         5.3989         3.8577        10.7111         4.9564        21.2802         6.0446
madar.test.lev.1.jo.en                 4.5098            N/A        15.6862         8.9035         8.1552         4.9318         3.4787         8.4273         4.6918        12.3063         4.4372         5.7071         5.9558        11.5897         5.4567        16.1228        12.5860
madar.test.lev.1.sy.en                 4.5497            N/A        17.9689        13.8372         6.6088         4.5557         2.9681         5.4535         4.4048         8.1860         3.4084         5.5772         3.8311         9.5880         4.7389        18.3300         5.4213
madar.test.mgr.0.dz.en                 6.1235            N/A         6.1400         2.0469         6.1833         1.2785         2.9461         8.2949         6.5493         8.0250         4.6725         7.0059         5.8055         8.2712         3.9203        11.4069         2.8238
madar.test.mgr.0.ly.en                 4.0111            N/A         9.6165         2.5794         6.1235         4.4999         1.5350         6.6960         3.9744         8.1351         2.7183         4.7450         4.5463         7.7097         3.9553         8.9969         1.2883
madar.test.mgr.0.ma.en                 3.3537            N/A        32.3032        10.4402         5.1399         0.8851         1.4398         6.2584         3.2004         7.8682         2.8072         4.5779        10.2473         6.9280         3.1389        13.6943         6.7394
madar.test.mgr.0.tn.en                 1.9891            N/A        34.7214        14.6195         3.2741         0.6060         1.5247         3.9584         2.2717         4.1509         1.9499         2.6855         5.2637         4.5879         2.1355         7.9380         3.1158
madar.test.mgr.1.ly.en                 5.8058            N/A        12.8304         3.7419         8.2071         4.9889         2.8741         8.8769         5.3362        10.9511         4.9669         6.6405         6.3486        11.2755         6.2532        13.1602         1.8590
madar.test.mgr.1.ma.en                 6.2198            N/A        12.8436         5.7868         6.9362         1.5615         2.6596         8.8652         6.6867         9.4332         5.5407         8.5362         9.8615         8.7944         4.9892        14.7528        13.4046
madar.test.mgr.1.tn.en                 1.4982            N/A         8.9953         6.2505         2.5847         0.5062         1.2237         3.7771         2.0826         3.4479         1.5950         2.2634         4.0665         3.5885         1.6090         6.2830         1.1880
madar.test.msa.0.ms.en                16.7796            N/A        26.1179        18.5380        16.4008        15.1388        10.6907        18.9449        17.5177        18.4081        13.6699        17.9485        19.7963        17.9171        15.2732        19.7871        18.3698
madar.test.nil.0.eg.en                 5.5243            N/A        45.4638        24.0705        10.6461         2.9976         3.3685        15.2746         4.8880        15.6873         4.3222        11.1710         5.3239        16.3548         8.8262         4.0892        15.4674
madar.test.nil.0.sd.en                 9.5002            N/A        12.9939         8.6220         9.6679         7.4233         5.5864        12.0760         9.7459        11.3658         6.1266        10.4769         9.3906        11.6716         7.7754        17.3772        16.0650
madar.test.nil.1.eg.en                 5.0809            N/A        14.6856        13.6482         9.1746         2.7308         2.8327        12.5620         5.1227        13.3048         3.9549         9.0071         4.5622        13.0012         6.1033         1.6192        15.2942
madar.test.nil.2.eg.en                 4.5075            N/A        17.2540        17.5213         9.4674         2.5920         2.3672        13.0643         3.7693        14.0239         4.4735         8.9733         4.1544        14.4328         7.1963         1.5818        14.0866



Dialect CHRF Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                 28.2025            N/A        35.0335        26.1885        29.6572        26.1045        24.5863        31.8148        28.5079        30.3787        24.8732        28.8800        30.9693        30.1580        26.3595        32.8423        31.6918
QAraC.test.glf.0.qa.en                28.5013            N/A        35.5752        26.6142        30.1994        27.0492        25.0430        32.1042        28.9532        30.8040        25.3705        29.3671        30.9378        30.5154        26.9786        33.1643        32.0481
bible.dev.mgr.0.ma.en                 21.2920            N/A        25.7067        16.7153        24.5501        10.9749        18.3916        22.0215        21.8656        26.3772        17.6334        19.5327        23.1366        27.6506        20.1873        31.4743        26.8161
bible.dev.mgr.0.tn.en                 21.2221            N/A        30.7248        24.8703        24.2104        10.7043        20.4296        25.9841        23.6756        26.6570        18.1225        21.6018        27.9895        27.2826        21.0841        30.1571        29.3386
bible.dev.msa.0.ms.en                 35.7136            N/A        33.8540        27.6350        32.1563        30.6828        28.1249        29.9870        35.6499        37.1242        23.8522        32.2658        35.6398        36.2217        31.7849        38.0651        37.3600
bible.dev.msa.1.ms.en                 29.2559            N/A        31.7022        22.6827        27.4421        26.5548        23.0680        27.8602        29.4099        31.5258        22.3612        27.2081        28.4862        32.5490        26.1274        31.5510        30.8202
bible.test.mgr.0.ma.en                21.2529            N/A        27.6929        18.2918        24.1187        10.5194        17.3360        21.4558        21.1396        26.7319        17.5861        19.3685        22.8017        27.6347        19.6141        31.6915        29.6195
bible.test.mgr.0.tn.en                19.5085            N/A        29.3863        23.7074        22.4329        10.3437        19.3402        23.9491        22.1900        25.5372        18.0148        20.9167        26.6125        25.7361        19.4504        29.9566        28.3570
bible.test.msa.0.ms.en                35.3407            N/A        34.1985        27.6136        31.4555        29.9455        27.3589        28.1086        34.6987        36.2975        24.1091        31.5848        34.5285        35.4619        30.6200        36.2138        36.4675
bible.test.msa.1.ms.en                28.5897            N/A        31.5836        21.7550        26.8951        26.0858        22.4362        25.8259        28.4000        30.8473        22.5547        26.9565        27.7617        31.1514        25.2908        30.1975        30.3060
madar.dev.glf.0.qa.en                 32.6983            N/A        48.6097        35.1437        34.6475        28.6760        28.0306        38.0974        32.5631        39.7009        30.1382        33.4503        34.4649        35.6717        32.2303        46.7167        38.1031
madar.dev.lev.0.lb.en                 29.0389            N/A        65.7268        49.7372        32.4360        21.0173        24.9355        35.7207        28.8472        38.5039        27.4341        29.6051        37.9259        38.5377        29.7307        44.0845        37.9461
madar.dev.mgr.0.ma.en                 22.7890            N/A        51.9157        30.9667        26.9770        11.9522        17.7019        28.5710        23.2506        31.4436        22.3084        25.1978        35.6937        32.6124        22.0765        36.7886        34.5711
madar.dev.mgr.0.tn.en                 22.8328            N/A        68.2109        49.9958        25.9857        10.1346        20.2949        28.5551        24.0205        29.3277        22.6876        24.2496        33.0959        29.1701        20.9989        37.4094        34.5573
madar.dev.msa.0.ms.en                 49.5045            N/A        59.5150        51.2059        48.6588        45.6222        40.2229        52.2251        49.8352        51.3139        44.2908        50.7070        52.5120        51.1725        47.0498        53.0367        51.3229
madar.dev.nil.0.eg.en                 29.3985            N/A        71.3965        49.7730        35.0536        24.7171        23.7877        40.8379        29.0185        39.7708        27.4804        35.0462        30.8071        41.2610        33.2139        19.7045        34.6892
madar.test.glf.0.iq.en                31.2207            N/A        41.7461        24.2933        33.5163        31.7639        28.0902        35.8944        32.6460        37.2559        29.9070        33.3822        34.8886        39.1871        24.8323        42.8224        40.4300
madar.test.glf.0.om.en                46.3453            N/A        37.6278        23.5763        44.5344        37.6304        32.0042        45.3289        46.1648        39.9943        39.8891        46.4857        45.0724        46.1842        42.2284        44.5217        45.2393
madar.test.glf.0.qa.en                32.7585            N/A        48.1713        35.6592        34.3598        29.2706        27.9087        38.4250        32.5841        40.0737        30.3243        33.4836        34.6703        35.7464        32.0228        46.6668        36.7357
madar.test.glf.0.sa.en                39.0225            N/A        46.8176        38.2483        38.8710        38.0443        33.7842        40.7803        39.5066        41.3759        36.5926        40.5537        39.1393        41.3732        39.0858        48.0038        37.6797
madar.test.glf.0.ye.en                31.4202            N/A        40.3760        24.2787        32.5718        29.9341        25.4741        32.7324        31.1343        34.7909        29.2702        31.7122        32.9043        35.0493        31.3101        40.4194        38.2395
madar.test.glf.1.iq.en                30.5524            N/A        41.7175        24.5671        33.5160        30.7799        27.6102        34.5435        31.7912        36.8909        29.2434        32.6886        31.5590        38.6684        24.4016        42.9084        40.6003
madar.test.glf.1.sa.en                31.5199            N/A        44.1085        34.7969        31.6180        31.1414        27.6169        32.6186        31.5484        33.1381        30.0688        32.2944        31.3373        33.2620        32.7949        39.1312        31.1110
madar.test.glf.2.iq.en                28.2893            N/A        33.5196        20.6304        30.2751        28.8046        26.3810        31.7540        29.8495        31.3183        27.4822        30.6615        30.3797        32.8838        22.4040        35.6748        34.8445
madar.test.lev.0.jo.en                31.2965            N/A        44.7050        39.6114        36.0594        30.9532        27.2578        37.8615        31.3492        42.6149        30.1687        32.3673         8.7269        42.4156        33.8197        47.9749        44.1841
madar.test.lev.0.lb.en                26.9486            N/A        61.0274        45.9299        30.7192        19.6353        23.7308        33.8803        27.2505        36.5032        26.5373        28.0031        36.9749        36.7205        28.2014        41.8583        33.7585
madar.test.lev.0.pa.en                30.7608            N/A        47.4072        37.3705        36.6082        29.3925        25.5441        34.4623        30.2515        42.1785        29.0511        32.0033        33.1902        37.8445        31.7246        49.3601        45.6334
madar.test.lev.0.sy.en                31.7417            N/A        49.1155        44.9515        34.7458        30.9853        25.7421        34.1644        32.0998        36.9827        29.7910        33.1558        31.1868        40.8749        32.5776        51.3337        38.9431
madar.test.lev.1.jo.en                32.0403            N/A        45.3928        38.8229        36.7093        31.6933        27.8272        37.7778        32.3620        41.4169        31.1248        33.5841        35.0181        41.4432        33.1547        47.6552        43.6253
madar.test.lev.1.sy.en                31.4553            N/A        47.9102        44.6970        34.4392        30.9527        26.1855        33.8217        31.5300        36.3509        29.3154        32.6497        30.8944        39.2398        31.9048        48.8072        37.1821
madar.test.mgr.0.dz.en                29.6926            N/A        29.1857        18.0996        29.7521        12.1530        23.6116        33.9642        30.7865        32.6824        28.1930        30.8080        29.5921        34.7355        24.0757        39.4434        25.1969
madar.test.mgr.0.ly.en                27.8002            N/A        38.4195        24.3264        31.7333        26.3715        20.3742        33.7458        28.0589        35.3416        25.5684        28.7274        30.2120        35.7272        26.7414        38.4487        21.7799
madar.test.mgr.0.ma.en                27.2103            N/A        54.2547        33.1593        30.1641        13.6783        19.6474        32.8829        27.6785        34.6712        26.1360        29.3042        40.1613        35.1433        25.5022        44.2573        36.5439
madar.test.mgr.0.tn.en                21.7450            N/A        58.2080        44.0745        25.4137        10.7662        19.8194        27.4218        23.1990        27.8784        21.9031        23.5544        31.1350        28.1545        19.8251        36.2168        28.4349
madar.test.mgr.1.ly.en                30.7710            N/A        40.0933        26.3252        34.5266        27.7852        22.7672        36.8625        31.3514        38.2853        29.3670        32.4128        33.9487        39.3885        29.3877        42.4731        23.9744
madar.test.mgr.1.ma.en                32.4894            N/A        40.0681        28.4948        32.8653        15.4670        22.3868        36.7482        33.2279        36.5736        30.8690        34.6284        39.0422        37.0783        28.8209        45.1714        44.9946
madar.test.mgr.1.tn.en                20.7913            N/A        38.2858        34.6645        24.8238        10.5520        19.5314        27.0285        22.9789        27.0940        21.6653        23.1059        28.7889        27.5685        20.5985        34.0932        20.2740
madar.test.msa.0.ms.en                48.3684            N/A        56.7095        49.0760        47.3642        45.2071        38.7964        51.1949        49.0232        50.3973        43.7026        49.4323        51.8311        50.2318        46.1168        52.1378        50.4100
madar.test.nil.0.eg.en                30.6934            N/A        66.8657        53.3728        37.5875        25.7148        24.9008        44.2014        29.8685        43.8769        28.6993        37.2103        31.7786        45.1984        35.5119        25.8377        42.9762
madar.test.nil.0.sd.en                37.2115            N/A        40.5817        36.1760        36.9250        33.9793        29.5762        41.1879        37.9779        38.9761        32.8961        38.3671        39.2801        40.0770        33.9854        46.7088        45.4156
madar.test.nil.1.eg.en                31.1775            N/A        46.3365        45.6470        37.1417        25.5066        24.5642        43.3338        30.2423        42.5320        28.9236        36.0170        31.0072        44.0711        33.5210        14.1722        46.2362
madar.test.nil.2.eg.en                28.6252            N/A        46.6583        46.0503        35.6293        24.8394        23.1663        41.3608        27.7885        41.5140        27.8186        34.6107        28.7409        42.8341        33.5906        13.5401        43.3264



Dialect COMET_Unbabel_XCOMET-XL Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  0.8124            N/A         0.7846         0.7241         0.7943         0.7407         0.7581         0.8147         0.8204         0.7818         0.7546         0.8123         0.8412         0.8305         0.7419         0.8216         0.8257
QAraC.test.glf.0.qa.en                 0.8116            N/A         0.7801         0.7229         0.7934         0.7398         0.7574         0.8141         0.8200         0.7819         0.7550         0.8103         0.8403         0.8282         0.7438         0.8201         0.8229
bible.dev.mgr.0.ma.en                  0.5383            N/A         0.3247         0.2830         0.3740         0.3412         0.3693         0.5018         0.5730         0.3755         0.4457         0.4961         0.5165         0.4039         0.4518         0.3841         0.4063
bible.dev.mgr.0.tn.en                  0.5513            N/A         0.3830         0.3488         0.4798         0.3545         0.4628         0.5582         0.6311         0.4627         0.4739         0.5769         0.5253         0.4790         0.4448         0.4916         0.4978
bible.dev.msa.0.ms.en                  0.8037            N/A         0.7452         0.6807         0.7294         0.7507         0.6471         0.7120         0.8002         0.8004         0.6137         0.7329         0.8033         0.7849         0.6968         0.8225         0.8105
bible.dev.msa.1.ms.en                  0.7810            N/A         0.7426         0.6540         0.7179         0.7367         0.6180         0.7176         0.7778         0.7913         0.6203         0.7187         0.7893         0.7784         0.6742         0.8012         0.7892
bible.test.mgr.0.ma.en                 0.5273            N/A         0.3255         0.2708         0.3573         0.3278         0.3350         0.4760         0.5392         0.3575         0.4471         0.4901         0.5048         0.3967         0.4176         0.3830         0.3986
bible.test.mgr.0.tn.en                 0.5231            N/A         0.3631         0.3360         0.4623         0.3478         0.4318         0.5393         0.5988         0.4389         0.4761         0.5649         0.5233         0.4746         0.4185         0.4857         0.4826
bible.test.msa.0.ms.en                 0.7919            N/A         0.7560         0.6811         0.7288         0.7347         0.6294         0.6935         0.7914         0.7980         0.6063         0.7318         0.7946         0.7783         0.6868         0.8087         0.8045
bible.test.msa.1.ms.en                 0.7716            N/A         0.7505         0.6506         0.7169         0.7227         0.5917         0.6980         0.7722         0.7847         0.6154         0.7146         0.7806         0.7673         0.6572         0.7875         0.7905
madar.dev.glf.0.qa.en                  0.9060            N/A         0.8529         0.7834         0.8751         0.8271         0.8491         0.9000         0.9111         0.8544         0.8636         0.9041         0.9228         0.9097         0.7969         0.8890         0.9028
madar.dev.lev.0.lb.en                  0.8466            N/A         0.8157         0.7732         0.7854         0.7009         0.8028         0.8500         0.8724         0.8131         0.8341         0.8635         0.8071         0.8067         0.7552         0.8296         0.8294
madar.dev.mgr.0.ma.en                  0.7718            N/A         0.6919         0.5959         0.6628         0.6022         0.6322         0.7441         0.7867         0.6518         0.7560         0.7392         0.6960         0.6514         0.6713         0.6944         0.6954
madar.dev.mgr.0.tn.en                  0.7536            N/A         0.7530         0.7010         0.7159         0.6039         0.7274         0.7798         0.8059         0.7197         0.7691         0.7975         0.7593         0.6895         0.6520         0.7389         0.7493
madar.dev.msa.0.ms.en                  0.9376            N/A         0.9434         0.9175         0.9252         0.9218         0.8970         0.9444         0.9391         0.9449         0.9016         0.9356         0.9545         0.9418         0.9217         0.9477         0.9410
madar.dev.nil.0.eg.en                  0.8795            N/A         0.8714         0.8414         0.8182         0.8006         0.8187         0.8595         0.8956         0.8576         0.8663         0.8506         0.9119         0.8582         0.8002         0.8626         0.8622
madar.test.glf.0.iq.en                 0.8609            N/A         0.7856         0.6615         0.8421         0.8552         0.8380         0.8878         0.8999         0.8354         0.8546         0.8863         0.9074         0.8302         0.7264         0.8451         0.8612
madar.test.glf.0.om.en                 0.9200            N/A         0.8254         0.7149         0.9023         0.8518         0.8164         0.9115         0.9263         0.8750         0.8789         0.9207         0.9332         0.9163         0.8938         0.8939         0.9013
madar.test.glf.0.qa.en                 0.9076            N/A         0.8542         0.7878         0.8751         0.8256         0.8501         0.9007         0.9116         0.8554         0.8643         0.9047         0.9240         0.9120         0.7977         0.8900         0.9048
madar.test.glf.0.sa.en                 0.9327            N/A         0.8707         0.8030         0.9233         0.9231         0.8963         0.9432         0.9371         0.9378         0.8967         0.9323         0.9483         0.9376         0.9009         0.9296         0.9278
madar.test.glf.0.ye.en                 0.9003            N/A         0.8389         0.7070         0.8532         0.8468         0.8046         0.9129         0.9079         0.8999         0.8624         0.8951         0.9125         0.8860         0.8506         0.8896         0.8891
madar.test.glf.1.iq.en                 0.8619            N/A         0.7903         0.6689         0.8439         0.8534         0.8386         0.8875         0.8993         0.8356         0.8525         0.8858         0.9051         0.8323         0.7281         0.8510         0.8640
madar.test.glf.1.sa.en                 0.9079            N/A         0.8560         0.7847         0.8970         0.8983         0.8682         0.9185         0.9113         0.9137         0.8705         0.9072         0.9254         0.9153         0.8769         0.9067         0.9047
madar.test.glf.2.iq.en                 0.8494            N/A         0.7587         0.6463         0.8312         0.8428         0.8260         0.8757         0.8881         0.8178         0.8428         0.8773         0.8967         0.8122         0.7169         0.8308         0.8497
madar.test.lev.0.jo.en                 0.9037            N/A         0.8165         0.7816         0.8467         0.8655         0.8585         0.8912         0.9108         0.8627         0.8795         0.9033         0.3230         0.8680         0.8527         0.8764         0.8884
madar.test.lev.0.lb.en                 0.8387            N/A         0.8035         0.7572         0.7797         0.6971         0.7974         0.8454         0.8669         0.8046         0.8306         0.8560         0.8017         0.8009         0.7519         0.8208         0.8211
madar.test.lev.0.pa.en                 0.8994            N/A         0.8185         0.7628         0.8334         0.8403         0.7964         0.9016         0.9062         0.8544         0.8624         0.8975         0.9098         0.8843         0.8664         0.8714         0.8748
madar.test.lev.0.sy.en                 0.8950            N/A         0.7948         0.7872         0.8520         0.8618         0.8120         0.9079         0.9042         0.8808         0.8609         0.8925         0.9182         0.8677         0.8376         0.8611         0.8643
madar.test.lev.1.jo.en                 0.8997            N/A         0.8131         0.7774         0.8438         0.8628         0.8557         0.8901         0.9069         0.8597         0.8764         0.9013         0.9017         0.8645         0.8485         0.8728         0.8856
madar.test.lev.1.sy.en                 0.8904            N/A         0.7870         0.7801         0.8494         0.8558         0.8087         0.9027         0.8979         0.8761         0.8536         0.8859         0.9115         0.8623         0.8335         0.8522         0.8582
madar.test.mgr.0.dz.en                 0.8561            N/A         0.6034         0.5415         0.7370         0.6459         0.7698         0.8423         0.8702         0.7692         0.8281         0.8619         0.8644         0.7200         0.6836         0.7727         0.7892
madar.test.mgr.0.ly.en                 0.8563            N/A         0.7590         0.6704         0.8083         0.7678         0.6989         0.8618         0.8804         0.8231         0.8243         0.8680         0.8849         0.8215         0.7345         0.8389         0.8251
madar.test.mgr.0.ma.en                 0.8196            N/A         0.7112         0.6168         0.6852         0.6259         0.6605         0.7848         0.8351         0.6767         0.8005         0.7802         0.7275         0.6765         0.7034         0.7224         0.7247
madar.test.mgr.0.tn.en                 0.7422            N/A         0.7257         0.6870         0.7063         0.6045         0.7143         0.7687         0.7944         0.7127         0.7572         0.7837         0.7470         0.6780         0.6464         0.7367         0.7443
madar.test.mgr.1.ly.en                 0.8667            N/A         0.7568         0.6754         0.8209         0.7779         0.7112         0.8710         0.8927         0.8319         0.8365         0.8789         0.8946         0.8327         0.7453         0.8471         0.8366
madar.test.mgr.1.ma.en                 0.8505            N/A         0.6704         0.6041         0.7031         0.6451         0.6874         0.8074         0.8657         0.6931         0.8262         0.8073         0.7355         0.6899         0.7239         0.7276         0.7378
madar.test.mgr.1.tn.en                 0.7422            N/A         0.6645         0.6585         0.7064         0.6068         0.7146         0.7727         0.7978         0.7122         0.7614         0.7883         0.7434         0.6717         0.6425         0.7257         0.7340
madar.test.msa.0.ms.en                 0.9379            N/A         0.9396         0.9157         0.9249         0.9201         0.8952         0.9428         0.9397         0.9438         0.9032         0.9345         0.9541         0.9416         0.9206         0.9471         0.9411
madar.test.nil.0.eg.en                 0.8860            N/A         0.8683         0.8485         0.8287         0.8103         0.8296         0.8687         0.9033         0.8671         0.8740         0.8583         0.9173         0.8670         0.8090         0.8737         0.8715
madar.test.nil.0.sd.en                 0.8988            N/A         0.8092         0.7848         0.8387         0.8369         0.8297         0.8804         0.9117         0.8377         0.8700         0.9009         0.9054         0.8631         0.8068         0.8765         0.8811
madar.test.nil.1.eg.en                 0.8936            N/A         0.8389         0.8431         0.8341         0.8173         0.8349         0.8772         0.9103         0.8732         0.8789         0.8638         0.9242         0.8725         0.8140         0.8815         0.8798
madar.test.nil.2.eg.en                 0.8756            N/A         0.8253         0.8281         0.8169         0.8036         0.8188         0.8628         0.8939         0.8579         0.8641         0.8482         0.9098         0.8570         0.8020         0.8656         0.8627



Dialect COMET_Unbabel_XCOMET-XL_ref-less Scores by Test Set
================================================================================

Test Set                      EuroLLM-9B-In..Falcon-H1-34B..Jais-2-70B-Chat Jais-2-8B-ChatLlama-3.3-70B..Mistral-Small..Qwen3-4B-Inst..aya-expanse-32b aya-expanse-8bc4ai-command-..c4ai-command-..c4ai-command-..command-a-tra.. gemma-3-27b-it  gemma-3-4b-it   gpt-4.1-mini   gpt-4.1-nano
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
QAraC.dev.glf.0.qa.en                  0.9078            N/A         0.8317         0.7900         0.8740         0.8247         0.8666         0.8814         0.9157         0.8453         0.8438         0.9007         0.9227         0.9126         0.8199         0.8869         0.9009
QAraC.test.glf.0.qa.en                 0.9082            N/A         0.8247         0.7893         0.8745         0.8234         0.8655         0.8810         0.9166         0.8447         0.8445         0.8998         0.9229         0.9108         0.8254         0.8855         0.8995
bible.dev.mgr.0.ma.en                  0.8343            N/A         0.3555         0.3334         0.4870         0.4631         0.5905         0.7033         0.8642         0.4452         0.6152         0.7315         0.6968         0.5059         0.7073         0.4399         0.4967
bible.dev.mgr.0.tn.en                  0.7669            N/A         0.4217         0.4103         0.6278         0.4542         0.6993         0.7308         0.8624         0.5728         0.5941         0.7798         0.6350         0.5942         0.6284         0.5813         0.6176
bible.dev.msa.0.ms.en                  0.8589            N/A         0.7598         0.7278         0.7614         0.7998         0.7369         0.7060         0.8577         0.8354         0.5872         0.7736         0.8115         0.8041         0.7740         0.8644         0.8607
bible.dev.msa.1.ms.en                  0.8578            N/A         0.7570         0.7306         0.7623         0.7990         0.7353         0.7057         0.8567         0.8345         0.5868         0.7757         0.8128         0.8044         0.7765         0.8628         0.8612
bible.test.mgr.0.ma.en                 0.8299            N/A         0.3484         0.3245         0.4615         0.4529         0.5608         0.6896         0.8461         0.4309         0.6334         0.7479         0.7004         0.4915         0.6689         0.4364         0.4867
bible.test.mgr.0.tn.en                 0.7294            N/A         0.3937         0.4018         0.6057         0.4637         0.6801         0.7078         0.8454         0.5454         0.6138         0.7870         0.6333         0.5949         0.5997         0.5834         0.5989
bible.test.msa.0.ms.en                 0.8491            N/A         0.7810         0.7328         0.7674         0.7818         0.7232         0.6854         0.8457         0.8372         0.5928         0.7813         0.8033         0.8060         0.7583         0.8494         0.8569
bible.test.msa.1.ms.en                 0.8490            N/A         0.7791         0.7277         0.7669         0.7841         0.7194         0.6862         0.8460         0.8367         0.5925         0.7801         0.8084         0.8079         0.7558         0.8493         0.8605
madar.dev.glf.0.qa.en                  0.9394            N/A         0.8423         0.7935         0.9002         0.8559         0.8958         0.9172         0.9430         0.8603         0.8965         0.9359         0.9519         0.9359         0.8235         0.8916         0.9241
madar.dev.lev.0.lb.en                  0.9173            N/A         0.7505         0.7445         0.8285         0.7568         0.8919         0.8934         0.9432         0.8386         0.9041         0.9328         0.8253         0.8321         0.8090         0.8364         0.8433
madar.dev.mgr.0.ma.en                  0.9295            N/A         0.6443         0.6050         0.7523         0.7160         0.7913         0.8489         0.9414         0.7011         0.9069         0.8691         0.7287         0.6958         0.7968         0.7023         0.7233
madar.dev.mgr.0.tn.en                  0.8849            N/A         0.6617         0.6612         0.8141         0.7123         0.8719         0.8796         0.9373         0.8030         0.8985         0.9276         0.8287         0.7609         0.7625         0.7730         0.8055
madar.dev.msa.0.ms.en                  0.9457            N/A         0.9326         0.9171         0.9339         0.9322         0.9202         0.9467         0.9454         0.9486         0.9060         0.9431         0.9561         0.9445         0.9341         0.9488         0.9453
madar.dev.nil.0.eg.en                  0.9264            N/A         0.7956         0.8191         0.8440         0.8460         0.8851         0.8679         0.9431         0.8676         0.9129         0.8813         0.9501         0.8630         0.8342         0.8571         0.8681
madar.test.glf.0.iq.en                 0.9030            N/A         0.7875         0.6832         0.8811         0.8958         0.8961         0.9234         0.9427         0.8555         0.8986         0.9292         0.9417         0.8438         0.7616         0.8530         0.8779
madar.test.glf.0.om.en                 0.9373            N/A         0.8281         0.7328         0.9206         0.8739         0.8501         0.9251         0.9432         0.8881         0.8980         0.9383         0.9467         0.9308         0.9149         0.8998         0.9102
madar.test.glf.0.qa.en                 0.9395            N/A         0.8429         0.7963         0.8993         0.8529         0.8942         0.9166         0.9436         0.8615         0.8970         0.9347         0.9509         0.9360         0.8226         0.8908         0.9238
madar.test.glf.0.sa.en                 0.9399            N/A         0.8551         0.7941         0.9306         0.9314         0.9163         0.9471         0.9439         0.9416         0.9031         0.9381         0.9541         0.9423         0.9091         0.9216         0.9234
madar.test.glf.0.ye.en                 0.9349            N/A         0.8410         0.7245         0.8842         0.8816         0.8536         0.9445         0.9426         0.9262         0.8988         0.9300         0.9367         0.9098         0.8837         0.8999         0.9054
madar.test.glf.1.iq.en                 0.9022            N/A         0.7886         0.6855         0.8800         0.8957         0.8958         0.9233         0.9427         0.8557         0.8987         0.9293         0.9406         0.8439         0.7626         0.8532         0.8768
madar.test.glf.1.sa.en                 0.9403            N/A         0.8547         0.7926         0.9306         0.9315         0.9158         0.9471         0.9441         0.9416         0.9033         0.9382         0.9542         0.9430         0.9083         0.9214         0.9227
madar.test.glf.2.iq.en                 0.9026            N/A         0.7883         0.6833         0.8800         0.8954         0.8958         0.9235         0.9426         0.8560         0.8988         0.9293         0.9405         0.8445         0.7635         0.8539         0.8773
madar.test.lev.0.jo.en                 0.9360            N/A         0.7985         0.7756         0.8640         0.8954         0.9095         0.9031         0.9409         0.8633         0.9107         0.9332         0.3551         0.8722         0.8780         0.8651         0.8874
madar.test.lev.0.lb.en                 0.9147            N/A         0.7457         0.7406         0.8317         0.7583         0.8924         0.8936         0.9437         0.8387         0.9071         0.9313         0.8252         0.8300         0.8093         0.8335         0.8450
madar.test.lev.0.pa.en                 0.9372            N/A         0.7927         0.7564         0.8543         0.8698         0.8442         0.9262         0.9434         0.8516         0.9013         0.9326         0.9334         0.8991         0.8985         0.8605         0.8717
madar.test.lev.0.sy.en                 0.9353            N/A         0.7603         0.7675         0.8795         0.8968         0.8665         0.9394         0.9435         0.9006         0.8988         0.9266         0.9490         0.8781         0.8706         0.8428         0.8559
madar.test.lev.1.jo.en                 0.9354            N/A         0.7973         0.7746         0.8631         0.8956         0.9072         0.9036         0.9410         0.8636         0.9109         0.9333         0.9187         0.8715         0.8765         0.8654         0.8858
madar.test.lev.1.sy.en                 0.9363            N/A         0.7598         0.7664         0.8796         0.8974         0.8674         0.9396         0.9435         0.9010         0.8988         0.9270         0.9493         0.8787         0.8706         0.8422         0.8578
madar.test.mgr.0.dz.en                 0.9309            N/A         0.6277         0.5773         0.7979         0.7197         0.8596         0.8993         0.9426         0.8130         0.9022         0.9335         0.9202         0.7509         0.7463         0.7913         0.8224
madar.test.mgr.0.ly.en                 0.9182            N/A         0.7642         0.6969         0.8589         0.8208         0.7730         0.8917         0.9425         0.8473         0.8843         0.9276         0.9315         0.8501         0.7814         0.8624         0.8532
madar.test.mgr.0.ma.en                 0.9302            N/A         0.6553         0.6101         0.7483         0.7178         0.7872         0.8476         0.9430         0.7001         0.9073         0.8676         0.7272         0.6947         0.7921         0.7028         0.7237
madar.test.mgr.0.tn.en                 0.8822            N/A         0.6584         0.6594         0.8123         0.7132         0.8699         0.8804         0.9390         0.8046         0.8987         0.9262         0.8300         0.7589         0.7630         0.7755         0.8051
madar.test.mgr.1.ly.en                 0.9175            N/A         0.7633         0.6980         0.8591         0.8199         0.7731         0.8917         0.9424         0.8472         0.8849         0.9275         0.9309         0.8497         0.7801         0.8633         0.8556
madar.test.mgr.1.ma.en                 0.9271            N/A         0.6438         0.6026         0.7482         0.7165         0.7858         0.8500         0.9419         0.7031         0.9019         0.8685         0.7222         0.6922         0.7874         0.7024         0.7225
madar.test.mgr.1.tn.en                 0.8782            N/A         0.6626         0.6648         0.8134         0.7135         0.8687         0.8777         0.9387         0.8005         0.8975         0.9255         0.8212         0.7520         0.7557         0.7695         0.8000
madar.test.msa.0.ms.en                 0.9464            N/A         0.9330         0.9177         0.9340         0.9328         0.9196         0.9468         0.9467         0.9477         0.9092         0.9428         0.9554         0.9459         0.9330         0.9488         0.9457
madar.test.nil.0.eg.en                 0.9242            N/A         0.8063         0.8201         0.8463         0.8455         0.8852         0.8672         0.9435         0.8674         0.9141         0.8797         0.9482         0.8636         0.8325         0.8595         0.8676
madar.test.nil.0.sd.en                 0.9286            N/A         0.8098         0.7980         0.8609         0.8611         0.8741         0.8935         0.9415         0.8443         0.8992         0.9305         0.9258         0.8750         0.8334         0.8723         0.8843
madar.test.nil.1.eg.en                 0.9233            N/A         0.8004         0.8163         0.8422         0.8436         0.8811         0.8660         0.9410         0.8636         0.9100         0.8752         0.9473         0.8578         0.8270         0.8559         0.8641
madar.test.nil.2.eg.en                 0.9226            N/A         0.7998         0.8162         0.8419         0.8443         0.8824         0.8658         0.9413         0.8637         0.9099         0.8756         0.9465         0.8571         0.8287         0.8566         0.8627


================================================================================
 Average BLEU Scores for Dialect Translations
================================================================================

Models sorted by average BLEU score:

Model: Jais-2-70B-Chat
  Average BLEU: 17.0044
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average BLEU: 10.2930
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average BLEU: 8.4893
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average BLEU: 8.3789
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average BLEU: 8.2144
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average BLEU: 7.8598
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average BLEU: 7.2957
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average BLEU: 6.3508
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average BLEU: 6.0517
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average BLEU: 5.9825
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average BLEU: 5.0244
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average BLEU: 4.9562
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average BLEU: 4.9349
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average BLEU: 3.9928
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average BLEU: 3.7730
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average BLEU: 2.9992
  Based on 42 test set(s)

================================================================================
 Average CHRF Scores for Dialect Translations
================================================================================

Models sorted by average CHRF score:

Model: Jais-2-70B-Chat
  Average CHRF: 44.4319
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average CHRF: 38.6355
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average CHRF: 36.5211
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average CHRF: 36.2330
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average CHRF: 36.0250
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average CHRF: 34.4547
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average CHRF: 33.8047
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average CHRF: 32.9720
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average CHRF: 32.6035
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average CHRF: 31.5986
  Based on 42 test set(s)

Model: aya-expanse-8b
  Average CHRF: 30.8161
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average CHRF: 30.4069
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average CHRF: 29.3080
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average CHRF: 27.6982
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average CHRF: 25.1283
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average CHRF: 25.0973
  Based on 42 test set(s)

================================================================================
 Average COMET_Unbabel_XCOMET-XL Scores for Dialect Translations
================================================================================

Models sorted by average COMET_Unbabel_XCOMET-XL score:

Model: aya-expanse-8b
  Average COMET_Unbabel_XCOMET-XL: 0.8453
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average COMET_Unbabel_XCOMET-XL: 0.8262
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average COMET_Unbabel_XCOMET-XL: 0.8205
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average COMET_Unbabel_XCOMET-XL: 0.8171
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average COMET_Unbabel_XCOMET-XL: 0.8166
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average COMET_Unbabel_XCOMET-XL: 0.8031
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average COMET_Unbabel_XCOMET-XL: 0.7994
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average COMET_Unbabel_XCOMET-XL: 0.7871
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average COMET_Unbabel_XCOMET-XL: 0.7838
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average COMET_Unbabel_XCOMET-XL: 0.7836
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average COMET_Unbabel_XCOMET-XL: 0.7695
  Based on 42 test set(s)

Model: Jais-2-70B-Chat
  Average COMET_Unbabel_XCOMET-XL: 0.7493
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average COMET_Unbabel_XCOMET-XL: 0.7419
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average COMET_Unbabel_XCOMET-XL: 0.7392
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average COMET_Unbabel_XCOMET-XL: 0.7380
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average COMET_Unbabel_XCOMET-XL: 0.6919
  Based on 42 test set(s)

================================================================================
 Average COMET_Unbabel_XCOMET-XL_ref-less Scores for Dialect Translations
================================================================================

Models sorted by average COMET_Unbabel_XCOMET-XL_ref-less score:

Model: aya-expanse-8b
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9242
  Based on 42 test set(s)

Model: EuroLLM-9B-Instruct
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.9038
  Based on 42 test set(s)

Model: c4ai-command-r7b-arabic-02-2025
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8900
  Based on 42 test set(s)

Model: aya-expanse-32b
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8634
  Based on 42 test set(s)

Model: command-a-translate-08-2025
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8593
  Based on 42 test set(s)

Model: c4ai-command-r-v01
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8422
  Based on 42 test set(s)

Model: Qwen3-4B-Instruct-2507
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8333
  Based on 42 test set(s)

Model: gpt-4.1-nano
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8328
  Based on 42 test set(s)

Model: gemma-3-27b-it
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8199
  Based on 42 test set(s)

Model: Llama-3.3-70B-Instruct
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8191
  Based on 42 test set(s)

Model: gpt-4.1-mini
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8169
  Based on 42 test set(s)

Model: c4ai-command-r-08-2024
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8165
  Based on 42 test set(s)

Model: gemma-3-4b-it
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.8063
  Based on 42 test set(s)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.7949
  Based on 42 test set(s)

Model: Jais-2-70B-Chat
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.7392
  Based on 42 test set(s)

Model: Jais-2-8B-Chat
  Average COMET_Unbabel_XCOMET-XL_ref-less: 0.7062
  Based on 42 test set(s)

================================================================================
 Ranking Statistics for Dialect BLEU Score
================================================================================
Total test sets with Dialect BLEU scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  4th: 1 time
  6th: 1 time
  7th: 1 time
  9th: 2 times
  10th: 7 times
  11th: 14 times
  12th: 6 times
  13th: 5 times
  14th: 4 times
  15th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 23 times
  2nd: 10 times
  3rd: 4 times
  4th: 1 time
  6th: 1 time
  7th: 1 time
  8th: 1 time
  14th: 1 time
  Win rate: 23/42 (54.8%)
  Top-3 rate: 37/42 (88.1%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  1st: 1 time
  2nd: 7 times
  3rd: 6 times
  5th: 2 times
  6th: 4 times
  7th: 1 time
  8th: 3 times
  11th: 1 time
  12th: 3 times
  13th: 1 time
  14th: 3 times
  15th: 5 times
  16th: 5 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 14/42 (33.3%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  5th: 2 times
  6th: 7 times
  7th: 8 times
  8th: 10 times
  9th: 9 times
  10th: 2 times
  11th: 2 times
  12th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  9th: 3 times
  10th: 2 times
  11th: 4 times
  12th: 6 times
  13th: 5 times
  14th: 5 times
  15th: 3 times
  16th: 14 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  12th: 1 time
  13th: 1 time
  14th: 5 times
  15th: 20 times
  16th: 15 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  2nd: 1 time
  3rd: 3 times
  4th: 5 times
  5th: 5 times
  6th: 8 times
  7th: 7 times
  8th: 9 times
  9th: 2 times
  11th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 4/42 (9.5%)

Model: aya-expanse-8b
  Total appearances: 42/42
  3rd: 1 time
  6th: 1 time
  8th: 3 times
  9th: 4 times
  10th: 8 times
  11th: 6 times
  12th: 9 times
  13th: 10 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 2 times
  3rd: 5 times
  4th: 10 times
  5th: 13 times
  6th: 7 times
  7th: 3 times
  8th: 1 time
  11th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 7/42 (16.7%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  10th: 2 times
  11th: 2 times
  12th: 4 times
  13th: 10 times
  14th: 14 times
  15th: 10 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  1st: 1 time
  4th: 1 time
  5th: 3 times
  6th: 1 time
  7th: 8 times
  8th: 9 times
  9th: 4 times
  10th: 10 times
  11th: 2 times
  13th: 2 times
  14th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 1/42 (2.4%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 1 time
  2nd: 2 times
  3rd: 2 times
  4th: 7 times
  5th: 2 times
  6th: 2 times
  7th: 2 times
  8th: 4 times
  9th: 2 times
  10th: 4 times
  11th: 4 times
  12th: 4 times
  13th: 3 times
  14th: 2 times
  16th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 5/42 (11.9%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  1st: 1 time
  2nd: 3 times
  3rd: 7 times
  4th: 7 times
  5th: 9 times
  6th: 7 times
  7th: 5 times
  9th: 3 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 11/42 (26.2%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  6th: 2 times
  8th: 1 time
  9th: 9 times
  10th: 7 times
  11th: 4 times
  12th: 7 times
  13th: 5 times
  14th: 5 times
  15th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 14 times
  2nd: 13 times
  3rd: 6 times
  4th: 2 times
  5th: 1 time
  7th: 1 time
  8th: 1 time
  14th: 1 time
  16th: 3 times
  Win rate: 14/42 (33.3%)
  Top-3 rate: 33/42 (78.6%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 1 time
  2nd: 4 times
  3rd: 8 times
  4th: 8 times
  5th: 5 times
  6th: 1 time
  7th: 5 times
  9th: 4 times
  14th: 1 time
  15th: 1 time
  16th: 4 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 13/42 (31.0%)

================================================================================
 Ranking Statistics for Dialect CHRF Score
================================================================================
Total test sets with Dialect CHRF scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  2nd: 1 time
  5th: 2 times
  6th: 1 time
  7th: 1 time
  8th: 1 time
  9th: 1 time
  10th: 7 times
  11th: 11 times
  12th: 14 times
  13th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 1/42 (2.4%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  1st: 20 times
  2nd: 13 times
  3rd: 3 times
  4th: 1 time
  5th: 1 time
  8th: 2 times
  10th: 1 time
  14th: 1 time
  Win rate: 20/42 (47.6%)
  Top-3 rate: 36/42 (85.7%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  2nd: 8 times
  3rd: 4 times
  6th: 3 times
  7th: 5 times
  8th: 2 times
  9th: 1 time
  12th: 2 times
  13th: 2 times
  14th: 5 times
  15th: 5 times
  16th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 12/42 (28.6%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  6th: 5 times
  7th: 8 times
  8th: 9 times
  9th: 11 times
  10th: 5 times
  11th: 2 times
  12th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  11th: 4 times
  12th: 5 times
  13th: 7 times
  14th: 10 times
  15th: 3 times
  16th: 13 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  13th: 1 time
  14th: 8 times
  15th: 17 times
  16th: 16 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  3rd: 4 times
  4th: 5 times
  5th: 7 times
  6th: 4 times
  7th: 8 times
  8th: 10 times
  9th: 1 time
  12th: 1 time
  13th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 4/42 (9.5%)

Model: aya-expanse-8b
  Total appearances: 42/42
  4th: 1 time
  6th: 4 times
  7th: 2 times
  9th: 6 times
  10th: 11 times
  11th: 7 times
  12th: 8 times
  13th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 1 time
  3rd: 5 times
  4th: 7 times
  5th: 10 times
  6th: 15 times
  7th: 3 times
  11th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  11th: 2 times
  12th: 5 times
  13th: 10 times
  14th: 9 times
  15th: 13 times
  16th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  1st: 1 time
  5th: 1 time
  6th: 1 time
  7th: 2 times
  8th: 10 times
  9th: 11 times
  10th: 9 times
  11th: 5 times
  12th: 2 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 1/42 (2.4%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  3rd: 4 times
  4th: 6 times
  5th: 3 times
  6th: 2 times
  7th: 9 times
  8th: 5 times
  9th: 4 times
  10th: 4 times
  11th: 1 time
  12th: 1 time
  13th: 1 time
  14th: 1 time
  16th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 4/42 (9.5%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  1st: 1 time
  2nd: 3 times
  3rd: 5 times
  4th: 15 times
  5th: 10 times
  6th: 3 times
  7th: 4 times
  8th: 1 time
  Win rate: 1/42 (2.4%)
  Top-3 rate: 9/42 (21.4%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  6th: 1 time
  9th: 6 times
  10th: 5 times
  11th: 9 times
  12th: 1 time
  13th: 13 times
  14th: 4 times
  15th: 2 times
  16th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 19 times
  2nd: 10 times
  3rd: 7 times
  5th: 1 time
  9th: 1 time
  14th: 1 time
  16th: 3 times
  Win rate: 19/42 (45.2%)
  Top-3 rate: 36/42 (85.7%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 1 time
  2nd: 6 times
  3rd: 10 times
  4th: 7 times
  5th: 7 times
  6th: 3 times
  8th: 2 times
  12th: 1 time
  14th: 3 times
  15th: 2 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 17/42 (40.5%)

================================================================================
 Ranking Statistics for Dialect COMET_Unbabel_XCOMET-XL Score
================================================================================
Total test sets with Dialect COMET_Unbabel_XCOMET-XL scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  2nd: 6 times
  3rd: 5 times
  4th: 13 times
  5th: 5 times
  6th: 8 times
  7th: 3 times
  9th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 11/42 (26.2%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  4th: 1 time
  5th: 1 time
  7th: 1 time
  8th: 6 times
  9th: 5 times
  10th: 1 time
  11th: 1 time
  12th: 4 times
  13th: 3 times
  14th: 11 times
  15th: 8 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  11th: 2 times
  12th: 2 times
  13th: 2 times
  14th: 9 times
  15th: 1 time
  16th: 26 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  7th: 1 time
  8th: 1 time
  9th: 5 times
  10th: 7 times
  11th: 13 times
  12th: 6 times
  13th: 4 times
  14th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  7th: 3 times
  8th: 1 time
  9th: 6 times
  10th: 2 times
  11th: 1 time
  12th: 5 times
  13th: 3 times
  14th: 5 times
  15th: 11 times
  16th: 5 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  9th: 2 times
  10th: 2 times
  11th: 5 times
  12th: 6 times
  13th: 10 times
  14th: 6 times
  15th: 7 times
  16th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  1st: 1 time
  2nd: 4 times
  3rd: 10 times
  4th: 10 times
  5th: 3 times
  6th: 5 times
  7th: 4 times
  8th: 1 time
  12th: 4 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 15/42 (35.7%)

Model: aya-expanse-8b
  Total appearances: 42/42
  1st: 16 times
  2nd: 12 times
  3rd: 4 times
  5th: 5 times
  6th: 2 times
  7th: 2 times
  8th: 1 time
  Win rate: 16/42 (38.1%)
  Top-3 rate: 32/42 (76.2%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  2nd: 1 time
  3rd: 5 times
  4th: 1 time
  5th: 2 times
  6th: 2 times
  8th: 3 times
  9th: 3 times
  10th: 6 times
  11th: 12 times
  12th: 7 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  3rd: 3 times
  4th: 4 times
  5th: 4 times
  6th: 4 times
  7th: 2 times
  8th: 7 times
  10th: 6 times
  11th: 2 times
  13th: 4 times
  15th: 4 times
  16th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 3/42 (7.1%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  2nd: 7 times
  3rd: 8 times
  4th: 3 times
  5th: 8 times
  6th: 2 times
  7th: 2 times
  8th: 2 times
  10th: 7 times
  11th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 15/42 (35.7%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 21 times
  2nd: 4 times
  3rd: 3 times
  4th: 4 times
  5th: 4 times
  6th: 2 times
  7th: 1 time
  10th: 2 times
  16th: 1 time
  Win rate: 21/42 (50.0%)
  Top-3 rate: 28/42 (66.7%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  2nd: 3 times
  3rd: 2 times
  4th: 1 time
  5th: 2 times
  6th: 3 times
  7th: 5 times
  8th: 1 time
  9th: 11 times
  10th: 2 times
  11th: 2 times
  12th: 3 times
  13th: 5 times
  14th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 5/42 (11.9%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  6th: 1 time
  7th: 1 time
  9th: 2 times
  10th: 3 times
  12th: 5 times
  13th: 11 times
  14th: 4 times
  15th: 11 times
  16th: 4 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 3 times
  2nd: 3 times
  4th: 4 times
  5th: 1 time
  6th: 5 times
  7th: 5 times
  8th: 12 times
  9th: 4 times
  10th: 4 times
  11th: 1 time
  Win rate: 3/42 (7.1%)
  Top-3 rate: 6/42 (14.3%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 1 time
  2nd: 2 times
  3rd: 2 times
  4th: 1 time
  5th: 7 times
  6th: 8 times
  7th: 12 times
  8th: 7 times
  9th: 2 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 5/42 (11.9%)

================================================================================
 Ranking Statistics for Dialect COMET_Unbabel_XCOMET-XL_ref-less Score
================================================================================
Total test sets with Dialect COMET_Unbabel_XCOMET-XL_ref-less scores: 42

Model: EuroLLM-9B-Instruct
  Total appearances: 42/42
  2nd: 8 times
  3rd: 16 times
  4th: 11 times
  5th: 4 times
  6th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 24/42 (57.1%)

Model: Jais-2-70B-Chat
  Total appearances: 42/42
  10th: 2 times
  11th: 1 time
  12th: 3 times
  13th: 1 time
  14th: 7 times
  15th: 19 times
  16th: 9 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Jais-2-8B-Chat
  Total appearances: 42/42
  13th: 2 times
  14th: 2 times
  15th: 12 times
  16th: 26 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Llama-3.3-70B-Instruct
  Total appearances: 42/42
  7th: 1 time
  8th: 12 times
  9th: 9 times
  10th: 3 times
  11th: 9 times
  12th: 4 times
  13th: 3 times
  14th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Mistral-Small-3.2-24B-Instruct-2506
  Total appearances: 42/42
  7th: 1 time
  8th: 12 times
  10th: 1 time
  11th: 4 times
  12th: 7 times
  13th: 7 times
  14th: 9 times
  15th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: Qwen3-4B-Instruct-2507
  Total appearances: 42/42
  5th: 7 times
  6th: 6 times
  7th: 7 times
  8th: 2 times
  9th: 1 time
  10th: 4 times
  12th: 4 times
  13th: 2 times
  14th: 9 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: aya-expanse-32b
  Total appearances: 42/42
  1st: 1 time
  2nd: 2 times
  3rd: 2 times
  4th: 7 times
  5th: 13 times
  6th: 4 times
  7th: 5 times
  8th: 3 times
  9th: 1 time
  15th: 4 times
  Win rate: 1/42 (2.4%)
  Top-3 rate: 5/42 (11.9%)

Model: aya-expanse-8b
  Total appearances: 42/42
  1st: 22 times
  2nd: 12 times
  3rd: 2 times
  4th: 4 times
  5th: 1 time
  6th: 1 time
  Win rate: 22/42 (52.4%)
  Top-3 rate: 36/42 (85.7%)

Model: c4ai-command-r-08-2024
  Total appearances: 42/42
  3rd: 2 times
  5th: 6 times
  6th: 3 times
  8th: 4 times
  9th: 4 times
  10th: 2 times
  11th: 7 times
  12th: 4 times
  13th: 9 times
  14th: 1 time
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: c4ai-command-r-v01
  Total appearances: 42/42
  3rd: 6 times
  4th: 7 times
  5th: 3 times
  6th: 6 times
  7th: 5 times
  9th: 2 times
  10th: 1 time
  11th: 2 times
  12th: 2 times
  14th: 2 times
  16th: 6 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 6/42 (14.3%)

Model: c4ai-command-r7b-arabic-02-2025
  Total appearances: 42/42
  2nd: 9 times
  3rd: 10 times
  4th: 4 times
  5th: 6 times
  6th: 5 times
  7th: 2 times
  9th: 4 times
  10th: 2 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 19/42 (45.2%)

Model: command-a-translate-08-2025
  Total appearances: 42/42
  1st: 15 times
  2nd: 5 times
  3rd: 2 times
  4th: 4 times
  6th: 6 times
  7th: 4 times
  9th: 2 times
  10th: 1 time
  12th: 2 times
  16th: 1 time
  Win rate: 15/42 (35.7%)
  Top-3 rate: 22/42 (52.4%)

Model: gemma-3-27b-it
  Total appearances: 42/42
  3rd: 2 times
  4th: 4 times
  5th: 1 time
  6th: 1 time
  7th: 6 times
  8th: 2 times
  9th: 2 times
  10th: 11 times
  11th: 3 times
  12th: 1 time
  13th: 6 times
  14th: 3 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 2/42 (4.8%)

Model: gemma-3-4b-it
  Total appearances: 42/42
  4th: 1 time
  6th: 4 times
  7th: 1 time
  8th: 2 times
  9th: 4 times
  10th: 2 times
  11th: 2 times
  12th: 7 times
  13th: 8 times
  14th: 5 times
  15th: 6 times
  Win rate: 0/42 (0.0%)
  Top-3 rate: 0/42 (0.0%)

Model: gpt-4.1-mini
  Total appearances: 42/42
  1st: 2 times
  2nd: 4 times
  7th: 4 times
  9th: 3 times
  10th: 2 times
  11th: 14 times
  12th: 8 times
  13th: 2 times
  14th: 3 times
  Win rate: 2/42 (4.8%)
  Top-3 rate: 6/42 (14.3%)

Model: gpt-4.1-nano
  Total appearances: 42/42
  1st: 2 times
  2nd: 2 times
  5th: 1 time
  6th: 3 times
  7th: 6 times
  8th: 5 times
  9th: 10 times
  10th: 11 times
  13th: 2 times
  Win rate: 2/42 (4.8%)
  Top-3 rate: 4/42 (9.5%)

================================================================================
 Average Rankings for Dialect BLEU Score
================================================================================
Lower rank is better (1st = best)

1. Jais-2-70B-Chat: 2.24 (based on 42 test set(s))
2. gpt-4.1-mini: 3.52 (based on 42 test set(s))
3. gemma-3-27b-it: 4.88 (based on 42 test set(s))
4. c4ai-command-r-08-2024: 4.90 (based on 42 test set(s))
5. gpt-4.1-nano: 6.19 (based on 42 test set(s))
6. aya-expanse-32b: 6.31 (based on 42 test set(s))
7. Llama-3.3-70B-Instruct: 7.98 (based on 42 test set(s))
8. command-a-translate-08-2025: 8.10 (based on 42 test set(s))
9. c4ai-command-r7b-arabic-02-2025: 8.38 (based on 42 test set(s))
10. Jais-2-8B-Chat: 8.45 (based on 42 test set(s))
11. aya-expanse-8b: 10.79 (based on 42 test set(s))
12. gemma-3-4b-it: 11.05 (based on 42 test set(s))
13. EuroLLM-9B-Instruct: 11.12 (based on 42 test set(s))
14. c4ai-command-r-v01: 13.48 (based on 42 test set(s))
15. Mistral-Small-3.2-24B-Instruct-2506: 13.50 (based on 42 test set(s))
16. Qwen3-4B-Instruct-2507: 15.12 (based on 42 test set(s))


================================================================================
 Average Rankings for Dialect CHRF Score
================================================================================
Lower rank is better (1st = best)

1. Jais-2-70B-Chat: 2.48 (based on 42 test set(s))
2. gpt-4.1-mini: 3.24 (based on 42 test set(s))
3. gemma-3-27b-it: 4.43 (based on 42 test set(s))
4. c4ai-command-r-08-2024: 5.17 (based on 42 test set(s))
5. gpt-4.1-nano: 5.33 (based on 42 test set(s))
6. aya-expanse-32b: 6.52 (based on 42 test set(s))
7. command-a-translate-08-2025: 7.33 (based on 42 test set(s))
8. Llama-3.3-70B-Instruct: 8.40 (based on 42 test set(s))
9. c4ai-command-r7b-arabic-02-2025: 8.90 (based on 42 test set(s))
10. Jais-2-8B-Chat: 9.07 (based on 42 test set(s))
11. aya-expanse-8b: 9.95 (based on 42 test set(s))
12. EuroLLM-9B-Instruct: 10.48 (based on 42 test set(s))
13. gemma-3-4b-it: 11.71 (based on 42 test set(s))
14. c4ai-command-r-v01: 13.83 (based on 42 test set(s))
15. Mistral-Small-3.2-24B-Instruct-2506: 14.00 (based on 42 test set(s))
16. Qwen3-4B-Instruct-2507: 15.14 (based on 42 test set(s))


================================================================================
 Average Rankings for Dialect COMET_Unbabel_XCOMET-XL Score
================================================================================
Lower rank is better (1st = best)

1. aya-expanse-8b: 2.64 (based on 42 test set(s))
2. command-a-translate-08-2025: 3.07 (based on 42 test set(s))
3. EuroLLM-9B-Instruct: 4.55 (based on 42 test set(s))
4. aya-expanse-32b: 4.95 (based on 42 test set(s))
5. c4ai-command-r7b-arabic-02-2025: 5.60 (based on 42 test set(s))
6. gpt-4.1-nano: 6.10 (based on 42 test set(s))
7. gpt-4.1-mini: 6.62 (based on 42 test set(s))
8. gemma-3-27b-it: 8.50 (based on 42 test set(s))
9. c4ai-command-r-v01: 8.69 (based on 42 test set(s))
10. c4ai-command-r-08-2024: 8.81 (based on 42 test set(s))
11. Llama-3.3-70B-Instruct: 11.12 (based on 42 test set(s))
12. Jais-2-70B-Chat: 11.69 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 12.57 (based on 42 test set(s))
14. Qwen3-4B-Instruct-2507: 13.05 (based on 42 test set(s))
15. gemma-3-4b-it: 13.07 (based on 42 test set(s))
16. Jais-2-8B-Chat: 14.98 (based on 42 test set(s))


================================================================================
 Average Rankings for Dialect COMET_Unbabel_XCOMET-XL_ref-less Score
================================================================================
Lower rank is better (1st = best)

1. aya-expanse-8b: 1.88 (based on 42 test set(s))
2. EuroLLM-9B-Instruct: 3.48 (based on 42 test set(s))
3. command-a-translate-08-2025: 4.26 (based on 42 test set(s))
4. c4ai-command-r7b-arabic-02-2025: 4.62 (based on 42 test set(s))
5. aya-expanse-32b: 6.10 (based on 42 test set(s))
6. c4ai-command-r-v01: 7.86 (based on 42 test set(s))
7. gpt-4.1-nano: 8.02 (based on 42 test set(s))
8. gemma-3-27b-it: 9.14 (based on 42 test set(s))
9. Qwen3-4B-Instruct-2507: 9.17 (based on 42 test set(s))
10. c4ai-command-r-08-2024: 9.48 (based on 42 test set(s))
11. gpt-4.1-mini: 9.60 (based on 42 test set(s))
12. Llama-3.3-70B-Instruct: 9.86 (based on 42 test set(s))
13. Mistral-Small-3.2-24B-Instruct-2506: 11.26 (based on 42 test set(s))
14. gemma-3-4b-it: 11.36 (based on 42 test set(s))
15. Jais-2-70B-Chat: 14.45 (based on 42 test set(s))
16. Jais-2-8B-Chat: 15.48 (based on 42 test set(s))


================================================================================
 Overall Scores (All Test Sets Concatenated) - Dialect
================================================================================

Model: EuroLLM-9B-Instruct
  BLEU: 5.3443
  CHRF: 30.6624
  Test sets: 42
  Sentences: 130226

Model: Jais-2-70B-Chat
  BLEU: 22.3574
  CHRF: 48.9640
  Test sets: 42
  Sentences: 130226

Model: Jais-2-8B-Chat
  BLEU: 8.9783
  CHRF: 36.3928
  Test sets: 42
  Sentences: 130226

Model: Llama-3.3-70B-Instruct
  BLEU: 7.1618
  CHRF: 33.1862
  Test sets: 42
  Sentences: 130226

Model: Mistral-Small-3.2-24B-Instruct-2506
  BLEU: 3.2752
  CHRF: 24.3377
  Test sets: 42
  Sentences: 130226

Model: Qwen3-4B-Instruct-2507
  BLEU: 3.4170
  CHRF: 25.4145
  Test sets: 42
  Sentences: 130226

Model: aya-expanse-32b
  BLEU: 8.2696
  CHRF: 35.6602
  Test sets: 42
  Sentences: 130226

Model: aya-expanse-8b
  BLEU: 5.5742
  CHRF: 31.1002
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r-08-2024
  BLEU: 9.3253
  CHRF: 36.7347
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r-v01
  BLEU: 4.5058
  CHRF: 28.2813
  Test sets: 42
  Sentences: 130226

Model: c4ai-command-r7b-arabic-02-2025
  BLEU: 6.8516
  CHRF: 32.2447
  Test sets: 42
  Sentences: 130226

Model: command-a-translate-08-2025
  BLEU: 6.9973
  CHRF: 34.6239
  Test sets: 42
  Sentences: 130226

Model: gemma-3-27b-it
  BLEU: 8.8715
  CHRF: 36.9186
  Test sets: 42
  Sentences: 130226

Model: gemma-3-4b-it
  BLEU: 5.6790
  CHRF: 29.7799
  Test sets: 42
  Sentences: 130226

Model: gpt-4.1-mini
  BLEU: 7.1703
  CHRF: 36.0926
  Test sets: 42
  Sentences: 130226

Model: gpt-4.1-nano
  BLEU: 6.9686
  CHRF: 36.3215
  Test sets: 42
  Sentences: 130226
